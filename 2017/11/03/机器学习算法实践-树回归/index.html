<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>机器学习算法实践-树回归 | PytLab</title>
  <meta name="author" content="PytLab">
  
  <meta name="description" content="Personal Blog of ShaoZhengjiang">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="机器学习算法实践-树回归"/>
  <meta property="og:site_name" content="PytLab"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <link rel="alternative" href="/true" title="PytLab" type="application/atom+xml">
  
  
    <link href="/assets/images/favicon/icon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/bootstrap.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-73223373-1', 'auto');
  ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/"></a>
      <div class="collapse navbar-collapse nav-menu">
		    <ul class="nav navbar-nav">
		      

          <!-- Categories -->
          
          <li>
            <a href="/" title="PytLab's Home" style="font-weight: normal; font-family: Calibri,Arial; font-size: 18px">
              <i class="fa fa-bank"></i>Home
            </a>
          </li>
          
		      

          <!-- Categories -->
          
          <!-- Archives -->
          <li class="dropdown">
            <a href="/archives" class="dropdown-toggle" data-toggle="dropdown" title="All the articles." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
            <i class="fa fa-archive"></i>Archives
            <b class="caret"></b>   
            </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/archives" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Archives</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/2018/01/27/实现属于自己的TensorFlow-三-反向传播与梯度下降算法实现/" style="font-size: 15px; font-family: 微软雅黑">实现属于自己的TensorFlow(三) - 反向传播...<span></span></a></li>
              
              <li><a href="/2018/01/25/实现属于自己的TensorFlow-二-梯度计算与反向传播/" style="font-size: 15px; font-family: 微软雅黑">实现属于自己的TensorFlow(二) - 梯度计算...<span></span></a></li>
              
              <li><a href="/2018/01/24/实现属于自己的TensorFlow-一-计算图与前向传播/" style="font-size: 15px; font-family: 微软雅黑">实现属于自己的TensorFlow(一) - 计算图与...<span></span></a></li>
              
              <li><a href="/2017/11/03/机器学习算法实践-树回归/" style="font-size: 15px; font-family: 微软雅黑">机器学习算法实践-树回归<span></span></a></li>
              
              <li><a href="/2017/10/27/机器学习实践-岭回归和LASSO回归/" style="font-size: 15px; font-family: 微软雅黑">机器学习算法实践-岭回归和LASSO<span></span></a></li>
              
              <li><a href="/2017/10/24/机器学习算法实践-标准与局部加权线性回归/" style="font-size: 15px; font-family: 微软雅黑">机器学习算法实践-标准与局部加权线性回归<span></span></a></li>
              
              <li><a href="/2017/10/15/机器学习算法实践-Platt-SMO和遗传算法优化SVM/" style="font-size: 15px; font-family: 微软雅黑">机器学习算法实践-Platt SMO和遗传算法优化SVM<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
          </li>

          
		      

          <!-- Categories -->
          
		      <li class="dropdown">
            <a href="/categories" class="dropdown-toggle" data-toggle="dropdown" title="All the categories." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
		    	  <i class="fa fa-folder"></i>Categories
            <b class="caret"></b>   
		    	  </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/categories" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Categories</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/categories/学习小结/" style="font-size: 15px; font-family: 微软雅黑">学习小结<span></span></a></li>
              
              <li><a href="/categories/学术/" style="font-size: 15px; font-family: 微软雅黑">学术<span></span></a></li>
              
              <li><a href="/categories/代码作品/" style="font-size: 15px; font-family: 微软雅黑">代码作品<span></span></a></li>
              
              <li><a href="/categories/教程/" style="font-size: 15px; font-family: 微软雅黑">教程<span></span></a></li>
              
              <li><a href="/categories/我的日常/" style="font-size: 15px; font-family: 微软雅黑">我的日常<span></span></a></li>
              
              <li><a href="/categories/译文/" style="font-size: 15px; font-family: 微软雅黑">译文<span></span></a></li>
              
              <li><a href="/categories/随笔/" style="font-size: 15px; font-family: 微软雅黑">随笔<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
		      </li>

          
		      

          <!-- Categories -->
          
          <!-- Tags -->
          <li class="dropdown">
            <a href="/tags" class="dropdown-toggle" data-toggle="dropdown" title="All the tags." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
            <i class="fa fa-tags"></i>Tags
            <b class="caret"></b>   
            </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/tags" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Tags</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/tags/python/" style="font-size: 15px; font-family: 微软雅黑">python<span></span></a></li>
              
              <li><a href="/tags/Cpp/" style="font-size: 15px; font-family: 微软雅黑">Cpp<span></span></a></li>
              
              <li><a href="/tags/catalysis/" style="font-size: 15px; font-family: 微软雅黑">catalysis<span></span></a></li>
              
              <li><a href="/tags/C/" style="font-size: 15px; font-family: 微软雅黑">C<span></span></a></li>
              
              <li><a href="/tags/chemistry/" style="font-size: 15px; font-family: 微软雅黑">chemistry<span></span></a></li>
              
              <li><a href="/tags/Parallel-Computing/" style="font-size: 15px; font-family: 微软雅黑">Parallel Computing<span></span></a></li>
              
              <li><a href="/tags/MPI/" style="font-size: 15px; font-family: 微软雅黑">MPI<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
          </li>
          
          
		      

          <!-- Categories -->
          
          <li>
            <a href="/about" title="About me." style="font-weight: normal; font-family: Calibri,Arial; font-size: 18px">
              <i class="fa fa-user"></i>About
            </a>
          </li>
          
		      
		    </ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">		
			<h1> 机器学习算法实践-树回归</h1>
		</div>		
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近由于开始要把精力集中在课题的应用上面了，这篇总结之后算法原理的学习先告一段落。本文主要介绍决策树用于回归问题的相关算法实现，其中包括回归树(regression tree)和模型树(model tree)的实现，并介绍了预剪枝(preprune)和后剪枝(postprune)的防止树过拟合的技术以及实现。最后对回归树和标准线性回归进行了对比。</p>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><p>在之前的文章中我总结了通过使用构建决策树来进行类型预测。直观来看树结构最容易对分类问题进行处理，通过递归我们在数据中选取最佳分割特征对训练数据进行分割并进行树分裂最终到达触底条件获得训练出来决策树，可以通过可视化的方式直观的查看训练模型并对数据进行分类。</p>
<a id="more"></a>
<p>通常决策树树分裂选择特征的方法有ID3, C4.5算法, C5.0算法和CART树。在《<a href="http://pytlab.github.io/2017/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5-%E5%86%B3%E7%AD%96%E6%A0%91/">机器学习算法实践-决策树(Decision Tree)</a>》中对ID3以及C4.5算法进行了介绍并使用ID3算法处理了分类问题。本文主要使用决策树解决回归问题，使用CART(Classification And Regression Trees)算法。</p>
<h3 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h3><p>CART是一种二分递归分割的技术，分割方法采用基于最小距离的基尼指数估计函数，将当前的样本集分为两个子样本集，使得生成的的每个非叶子节点都有两个分支。因此，CART算法生成的决策树是结构简洁的二叉树。</p>
<p>分类树是针对目标变量是离散型变量，通过二叉树将数据进行分割成离散类的方法。而回归树则是针对目标变量是连续性的变量，通过选取最优分割特征的某个值，然后数据根据大于或者小于这个值进行划分进行树分裂最终生成回归树。</p>
<h3 id="特征和最佳分割点的选取"><a href="#特征和最佳分割点的选取" class="headerlink" title="特征和最佳分割点的选取"></a>特征和最佳分割点的选取</h3><p>在使用决策树解决回归问题中我们需要不断的选取某一特征的一个值作为分割点来生成子树。选取的标准就是使得被分割的两部分数据能有最好的纯度。</p>
<ul>
<li>对于离散型数据我们可以通过计算分割两部分数据的基尼不纯度的变化来判定最有分割点；</li>
<li>对于连续性变量我们通过计算最小平方残差，也就是选择使得分割后数据方差变得最小的特征和分割点。直观的理解就是使得分割的两部分数据能够有最相近的值。</li>
</ul>
<h3 id="树分裂的终止条件"><a href="#树分裂的终止条件" class="headerlink" title="树分裂的终止条件"></a>树分裂的终止条件</h3><p>有了选取分割特征和最佳分割点的方法，树便可以依此进行分裂，但是分裂的终止条件是什么呢?</p>
<ol>
<li><strong>节点中所有目标变量的值相同</strong>, 既然都已经是相同的值了自然没有必要在分裂了，直接返回这个值就好了.</li>
<li>树的深度达到了预先指定的最大值</li>
<li><strong>不纯度的减小量小于预先定好的阈值</strong>,也就是之进一步的分割数据并不能更好的降低数据的不纯度的时候就可以停止树分裂了。</li>
<li>节点的数据量小于预先定好的阈值</li>
</ol>
<h3 id="回归树的Python实现"><a href="#回归树的Python实现" class="headerlink" title="回归树的Python实现"></a>回归树的Python实现</h3><p>本部分使用Python实现简单的回归树，并对给定的数据进行回归并可视化回归曲线和树结构。完整代码详见: <a href="https://github.com/PytLab/MLBox/tree/master/classification_and_regression_trees" target="_blank" rel="noopener">https://github.com/PytLab/MLBox/tree/master/classification_and_regression_trees</a></p>
<p>首先是加载数据的部分，这里的所有测试数据我均使用的《Machine Learning in Action》中的数据，格式比较规整加载方式也比较一致, 这里由于做树回归，自变量和因变量都放在同一个二维数组中:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(filename)</span>:</span></div><div class="line">    <span class="string">''' 加载文本文件中的数据.</span></div><div class="line">    '''</div><div class="line">    dataset = []</div><div class="line">    <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</div><div class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</div><div class="line">            line_data = [float(data) <span class="keyword">for</span> data <span class="keyword">in</span> line.split()]</div><div class="line">            dataset.append(line_data)</div><div class="line">    <span class="keyword">return</span> dataset</div></pre></td></tr></table></figure></p>
<p>树回归中再找到分割特征和分割值之后需要将数据进行划分以便构建子树或者叶子节点:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_dataset</span><span class="params">(dataset, feat_idx, value)</span>:</span></div><div class="line">    <span class="string">''' 根据给定的特征编号和特征值对数据集进行分割</span></div><div class="line">    '''</div><div class="line">    ldata, rdata = [], []</div><div class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataset:</div><div class="line">        <span class="keyword">if</span> data[feat_idx] &lt; value:</div><div class="line">            ldata.append(data)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            rdata.append(data)</div><div class="line">    <span class="keyword">return</span> ldata, rdata</div></pre></td></tr></table></figure>
<p>然后就是重要的选取最佳分割特征和分割值了，这里我们通过找打使得分割后的方差最小的分割点最为最佳分割点:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_best_feature</span><span class="params">(dataset, fleaf, ferr, opt)</span>:</span></div><div class="line">    <span class="string">''' 选取最佳分割特征和特征值</span></div><div class="line"></div><div class="line">    dataset: 待划分的数据集</div><div class="line">    fleaf: 创建叶子节点的函数</div><div class="line">    ferr: 计算数据误差的函数</div><div class="line">    opt: 回归树参数.</div><div class="line">        err_tolerance: 最小误差下降值;</div><div class="line">        n_tolerance: 数据切分最小样本数</div><div class="line">    '''</div><div class="line">    dataset = np.array(dataset)</div><div class="line">    m, n = dataset.shape</div><div class="line">    err_tolerance, n_tolerance = opt[<span class="string">'err_tolerance'</span>], opt[<span class="string">'n_tolerance'</span>]</div><div class="line"></div><div class="line">    err = ferr(dataset)</div><div class="line">    best_feat_idx, best_feat_val, best_err = <span class="number">0</span>, <span class="number">0</span>, float(<span class="string">'inf'</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 遍历所有特征</span></div><div class="line">    <span class="keyword">for</span> feat_idx <span class="keyword">in</span> range(n<span class="number">-1</span>):</div><div class="line">        values = dataset[:, feat_idx]</div><div class="line">        <span class="comment"># 遍历所有特征值</span></div><div class="line">        <span class="keyword">for</span> val <span class="keyword">in</span> values:</div><div class="line">            <span class="comment"># 按照当前特征和特征值分割数据</span></div><div class="line">            ldata, rdata = split_dataset(dataset.tolist(), feat_idx, val)</div><div class="line">            <span class="keyword">if</span> len(ldata) &lt; n_tolerance <span class="keyword">or</span> len(rdata) &lt; n_tolerance:</div><div class="line">                <span class="comment"># 如果切分的样本量太小</span></div><div class="line">                <span class="keyword">continue</span></div><div class="line"></div><div class="line">            <span class="comment"># 计算误差</span></div><div class="line">            new_err = ferr(ldata) + ferr(rdata)</div><div class="line">            <span class="keyword">if</span> new_err &lt; best_err:</div><div class="line">                best_feat_idx = feat_idx</div><div class="line">                best_feat_val = val</div><div class="line">                best_err = new_err</div><div class="line"></div><div class="line">    <span class="comment"># 如果误差变化并不大归为一类</span></div><div class="line">    <span class="keyword">if</span> abs(err - best_err) &lt; err_tolerance:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span>, fleaf(dataset)</div><div class="line"></div><div class="line">    <span class="comment"># 检查分割样本量是不是太小</span></div><div class="line">    ldata, rdata = split_dataset(dataset.tolist(), best_feat_idx, best_feat_val)</div><div class="line">    <span class="keyword">if</span> len(ldata) &lt; n_tolerance <span class="keyword">or</span> len(rdata) &lt; n_tolerance:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span>, fleaf(dataset)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> best_feat_idx, best_feat_val</div></pre></td></tr></table></figure></p>
<p>其中，停止选取的条件有两个: 一个是当分割的子数据集的大小小于一定值；一个是当选取的最佳分割点分割的数据的方差减小量小于一定的值。</p>
<p><code>fleaf</code>是创建叶子节点的函数引用，不同的树结构此函数也是不同的，例如本部分的回归树，创建叶子节点就是根据分割后的数据集平均值，而对于模型树来说，此函数返回值是根据数据集得到的回归系数。<code>ferr</code>是计算数据集不纯度的函数，不同的树模型该函数也会不同，对于回归树，此函数计算数据集的方差来判定数据集的纯度，而对于模型树来说我们需要计算线性模型拟合程度也就是线性模型的残差平方和。</p>
<p>然后就是最主要的回归树的生成函数了，树结构肯定需要通过递归创建的，选不出新的分割点的时候就触底：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(dataset, fleaf, ferr, opt=None)</span>:</span></div><div class="line">    <span class="string">''' 递归创建树结构</span></div><div class="line"></div><div class="line">    dataset: 待划分的数据集</div><div class="line">    fleaf: 创建叶子节点的函数</div><div class="line">    ferr: 计算数据误差的函数</div><div class="line">    opt: 回归树参数.</div><div class="line">        err_tolerance: 最小误差下降值;</div><div class="line">        n_tolerance: 数据切分最小样本数</div><div class="line">    '''</div><div class="line">    <span class="keyword">if</span> opt <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        opt = &#123;<span class="string">'err_tolerance'</span>: <span class="number">1</span>, <span class="string">'n_tolerance'</span>: <span class="number">4</span>&#125;</div><div class="line"></div><div class="line">    <span class="comment"># 选择最优化分特征和特征值</span></div><div class="line">    feat_idx, value = choose_best_feature(dataset, fleaf, ferr, opt)</div><div class="line">    </div><div class="line">    <span class="comment"># 触底条件</span></div><div class="line">    <span class="keyword">if</span> feat_idx <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> value</div><div class="line"></div><div class="line">    <span class="comment"># 创建回归树</span></div><div class="line">    tree = &#123;<span class="string">'feat_idx'</span>: feat_idx, <span class="string">'feat_val'</span>: value&#125;</div><div class="line"></div><div class="line">    <span class="comment"># 递归创建左子树和右子树</span></div><div class="line">    ldata, rdata = split_dataset(dataset, feat_idx, value)</div><div class="line">    ltree = create_tree(ldata, fleaf, ferr, opt)</div><div class="line">    rtree = create_tree(rdata, fleaf, ferr, opt)</div><div class="line">    tree[<span class="string">'left'</span>] = ltree</div><div class="line">    tree[<span class="string">'right'</span>] = rtree</div><div class="line"></div><div class="line">    <span class="keyword">return</span> tree</div></pre></td></tr></table></figure></p>
<h3 id="使用回归树对数据进行回归"><a href="#使用回归树对数据进行回归" class="headerlink" title="使用回归树对数据进行回归"></a>使用回归树对数据进行回归</h3><p>这里使用了现成的分段数据作为训练数据生成回归树，本文所有使用的数据详见: <a href="https://github.com/PytLab/MLBox/tree/master/classification_and_regression_trees" target="_blank" rel="noopener">https://github.com/PytLab/MLBox/tree/master/classification_and_regression_trees</a></p>
<h4 id="可视化数据点"><a href="#可视化数据点" class="headerlink" title="可视化数据点"></a>可视化数据点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">dataset = load_data(<span class="string">'ex0.txt'</span>)</div><div class="line">dataset = np.array(dataset)</div><div class="line"><span class="comment"># 绘制散点</span></div><div class="line">plt.scatter(dataset[:, <span class="number">0</span>], dataset[:, <span class="number">1</span>])</div></pre></td></tr></table></figure>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/ex0_data.png" alt=""></p>
<h4 id="创建回归树并可视化"><a href="#创建回归树并可视化" class="headerlink" title="创建回归树并可视化"></a>创建回归树并可视化</h4><p>看到这种分段的数据，回归树拟合它可是最合适不过了，我们创建回归树:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tree = create_tree(dataset, fleaf, ferr, opt=&#123;<span class="string">'n_tolerance'</span>: <span class="number">4</span>,</div><div class="line">                                              <span class="string">'err_tolerance'</span>: <span class="number">1</span>&#125;)</div></pre></td></tr></table></figure></p>
<p>通过Python字典表示的回归树结构:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'feat_idx'</span>: <span class="number">0</span>,</div><div class="line"> <span class="string">'feat_val'</span>: <span class="number">0.40015800000000001</span>,</div><div class="line"> <span class="string">'left'</span>: &#123;<span class="string">'feat_idx'</span>: <span class="number">0</span>,</div><div class="line">          <span class="string">'feat_val'</span>: <span class="number">0.20819699999999999</span>,</div><div class="line">          <span class="string">'left'</span>: <span class="number">-0.023838155555555553</span>,</div><div class="line">          <span class="string">'right'</span>: <span class="number">1.0289583666666666</span>&#125;,</div><div class="line"> <span class="string">'right'</span>: &#123;<span class="string">'feat_idx'</span>: <span class="number">0</span>,</div><div class="line">           <span class="string">'feat_val'</span>: <span class="number">0.609483</span>,</div><div class="line">           <span class="string">'left'</span>: <span class="number">1.980035071428571</span>,</div><div class="line">           <span class="string">'right'</span>: &#123;<span class="string">'feat_idx'</span>: <span class="number">0</span>,</div><div class="line">                     <span class="string">'feat_val'</span>: <span class="number">0.81674199999999997</span>,</div><div class="line">                     <span class="string">'left'</span>: <span class="number">2.9836209534883724</span>,</div><div class="line">                     <span class="string">'right'</span>: <span class="number">3.9871631999999999</span>&#125;&#125;&#125;</div></pre></td></tr></table></figure></p>
<p>这里我还是使用Graphviz来可视化回归树，类似之前决策树做分类的文章中的<code>dotify</code>函数，这里稍微修改下叶子节点的label，我们便可以递归得到决策树对应的dot文件, <code>dotify</code>函数的实现见:<a href="https://github.com/PytLab/MLBox/blob/master/classification_and_regression_trees/regression_tree.py#L159" target="_blank" rel="noopener">https://github.com/PytLab/MLBox/blob/master/classification_and_regression_trees/regression_tree.py#L159</a><br>然后获取树结构图:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">datafile = <span class="string">'ex0.txt'</span></div><div class="line">dotfile = <span class="string">'&#123;&#125;.dot'</span>.format(datafile.split(<span class="string">'.'</span>)[<span class="number">0</span>])</div><div class="line"><span class="keyword">with</span> open(dotfile, <span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">    content = dotify(tree)</div><div class="line">    f.write(content)</div></pre></td></tr></table></figure></p>
<p>生成回归树图片:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dot -Tpng ex0.dot -o ex0_tree.png</div></pre></td></tr></table></figure></p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/ex0_tree.png" alt=""></p>
<p>其中节点上数字代表:<code>特征编号: 特征分割值</code></p>
<h4 id="绘制回归树回归曲线"><a href="#绘制回归树回归曲线" class="headerlink" title="绘制回归树回归曲线"></a>绘制回归树回归曲线</h4><p>有了回归树，我们便可以绘制回归树回归曲线，看看它对于分段数据是否能有较好的回归效果：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 绘制回归曲线</span></div><div class="line">x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">50</span>)</div><div class="line">y = [tree_predict([i], tree) <span class="keyword">for</span> i <span class="keyword">in</span> x]</div><div class="line">plt.plot(x, y, c=<span class="string">'r'</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/ex0_regression.png" alt=""></p>
<h3 id="树剪枝"><a href="#树剪枝" class="headerlink" title="树剪枝"></a>树剪枝</h3><p>在介绍树剪枝之前先使用上一部分的代码对两组类似的数据进行回归，可视化后的数据以及回归曲线如下(<a href="https://github.com/PytLab/MLBox/blob/master/classification_and_regression_trees/ex00.txt" target="_blank" rel="noopener">数据文件左</a>&amp;<a href="https://github.com/PytLab/MLBox/blob/master/classification_and_regression_trees/ex2.txt" target="_blank" rel="noopener">数据文件右</a>):</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/prune_compare.png" alt=""></p>
<p>左右两边的数据的分布基本相同但是使用相同的参数得到的回归树却完全不同左边的回归树只有两个分支，而右边的分支则有很多，甚至有时候会为所有的数据点得到一个分支，这样回归树将会非常的庞大, 如下是可视化得到的两个回归树:</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/prune_tree_compare.png" alt=""></p>
<p>如果一棵树的节点过多则表明该模型可能对数据进行了“过拟合”。那么我们需要降低决策树的复杂度来避免过拟合，此过程就是<strong>剪枝</strong>。剪枝技术又分为<strong>预剪枝</strong>和<strong>后剪枝</strong>。</p>
<h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p>预剪枝是在生成决策树之前通过改变参数然后在树生成的过程中进行的。比如在上文中我们创建回归树的函数中有个<code>opt</code>参数，其中包含<code>n_tolerance</code>和<code>err_tolerance</code>，他们可以控制何时停止树的分裂，当增大叶子节点的最小数据量以及增大误差容忍度，树的分裂也会越提前的终止。当我们把误差变化容忍度增加到2000的时候得到的回归树以及回归曲线可视化如下:</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/pruned_tree_regression.png" alt=""></p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>预剪枝技术需要用于预先指定参数，但是后剪枝技术则是通过测试数据来自动进行剪枝不需要用户干预因此是一种更理想的剪枝技术，但是我们需要写剪枝函数来处理。</p>
<blockquote>
<p>后剪枝的大致思想就是我们针对一颗子树，尝试将其左右子树(节点)合并，通过测试数据计算合并前后的方差，如果合并后的方差比合并前的小，这说明可以合并此子树。</p>
</blockquote>
<p>对树进行塌陷处理: 我们对一棵树进行塌陷处理，就是递归将这棵树进行合并返回这棵树的平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">collapse</span><span class="params">(tree)</span>:</span></div><div class="line">    <span class="string">''' 对一棵树进行塌陷处理, 得到给定树结构的平均值</span></div><div class="line">    '''</div><div class="line">    <span class="keyword">if</span> not_tree(tree):</div><div class="line">        <span class="keyword">return</span> tree</div><div class="line">    ltree, rtree = tree[<span class="string">'left'</span>], tree[<span class="string">'right'</span>]</div><div class="line">    <span class="keyword">return</span> (collapse(ltree) + collapse(rtree))/<span class="number">2</span></div></pre></td></tr></table></figure>
<p>后剪枝的Python实现:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">postprune</span><span class="params">(tree, test_data)</span>:</span></div><div class="line">    <span class="string">''' 根据测试数据对树结构进行后剪枝</span></div><div class="line">    '''</div><div class="line">    <span class="keyword">if</span> not_tree(tree):</div><div class="line">        <span class="keyword">return</span> tree</div><div class="line"></div><div class="line">    <span class="comment"># 若没有测试数据则直接返回树平均值</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> test_data:</div><div class="line">        <span class="keyword">return</span> collapse(tree)</div><div class="line"></div><div class="line">    ltree, rtree = tree[<span class="string">'left'</span>], tree[<span class="string">'right'</span>]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> not_tree(ltree) <span class="keyword">and</span> not_tree(rtree):</div><div class="line">        <span class="comment"># 分割数据用于测试</span></div><div class="line">        ldata, rdata = split_dataset(test_data, tree[<span class="string">'feat_idx'</span>], tree[<span class="string">'feat_val'</span>])</div><div class="line">        <span class="comment"># 分别计算合并前和合并后的测试数据误差</span></div><div class="line">        err_no_merge = (np.sum((np.array(ldata) - ltree)**<span class="number">2</span>) +</div><div class="line">                        np.sum((np.array(rdata) - rtree)**<span class="number">2</span>))</div><div class="line">        err_merge = np.sum((np.array(test_data) - (ltree + rtree)/<span class="number">2</span>)**<span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="keyword">if</span> err_merge &lt; err_no_merge:</div><div class="line">            print(<span class="string">'merged'</span>)</div><div class="line">            <span class="keyword">return</span> (ltree + rtree)/<span class="number">2</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> tree</div><div class="line"></div><div class="line">    tree[<span class="string">'left'</span>] = postprune(tree[<span class="string">'left'</span>], test_data)</div><div class="line">    tree[<span class="string">'right'</span>] = postprune(tree[<span class="string">'right'</span>], test_data)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> tree</div></pre></td></tr></table></figure></p>
<p>我们看一下不对刚才的树进行预剪枝而是使用测试数据进行后剪枝的效果:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">data_test = load_data(<span class="string">'ex2test.txt'</span>)</div><div class="line">pruned_tree = postprune(tree, data_test)</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">merged</div><div class="line">merged</div><div class="line">merged</div><div class="line">merged</div><div class="line">merged</div><div class="line">merged</div><div class="line">merged</div><div class="line">merged</div></pre></td></tr></table></figure>
<p>通过输出可以看到总共进行了8次剪枝操作，通过把剪枝前和剪枝后的树可视化对比下看看:</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/postprune_compare.png" alt=""></p>
<p>树的规模的确是减小了。</p>
<h3 id="模型树"><a href="#模型树" class="headerlink" title="模型树"></a>模型树</h3><p>上一部分叶子节点上放的是分割后数据的平均值并以他作为满足条件的样本的预测值，本部分我们将在叶子节点上放一个线性模型来做预测。也就是指我们的树是由多个线性模型组成的，显然会比强行用平均值来建模更有优势。</p>
<ul>
<li>模型树使用多个线性函数来做回归比用多个平均值组成一棵大树的模型更有可解释性</li>
<li>而且线性模型的使用可以使树的规模减小，毕竟平均值的覆盖范围只是局部的，而线性模型可以覆盖所有具有线性关系的数据。</li>
<li>模型树也具有更高的预测准确度</li>
</ul>
<h4 id="创建模型树"><a href="#创建模型树" class="headerlink" title="创建模型树"></a>创建模型树</h4><p>模型树和回归树的思想是完全一致的，只是在生成叶子节点的方法以及计算数据误差(不纯度)的方式不同。在模型树里针对一个叶子节点我们需要使用分割到的数据进行线性回归得到线性回归系数而不是简单的计算数据的平均值。不纯度的计算也不是简单的计算数据的方差，而是计算线性模型的残差平方和。</p>
<p>为了能为叶子节点计算线性模型，我们还需要实现一个标准线性回归函数<code>linear_regression</code>, 相应模型树的<code>ferr</code>和<code>fleaf</code>的Python实现<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression</span><span class="params">(dataset)</span>:</span></div><div class="line">    <span class="string">''' 获取标准线性回归系数</span></div><div class="line">    '''</div><div class="line">    dataset = np.matrix(dataset)</div><div class="line">    <span class="comment"># 分割数据并添加常数列</span></div><div class="line">    X_ori, y = dataset[:, :<span class="number">-1</span>], dataset[:, <span class="number">-1</span>]</div><div class="line">    X_ori, y = np.matrix(X_ori), np.matrix(y)</div><div class="line">    m, n = X_ori.shape</div><div class="line">    X = np.matrix(np.ones((m, n+<span class="number">1</span>)))</div><div class="line">    X[:, <span class="number">1</span>:] = X_ori</div><div class="line"></div><div class="line">    <span class="comment"># 回归系数</span></div><div class="line">    w = (X.T*X).I*X.T*y</div><div class="line">    <span class="keyword">return</span> w, X, y</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fleaf</span><span class="params">(dataset)</span>:</span></div><div class="line">    <span class="string">''' 计算给定数据集的线性回归系数</span></div><div class="line">    '''</div><div class="line">    w, _, _ = linear_regression(dataset)</div><div class="line">    <span class="keyword">return</span> w</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ferr</span><span class="params">(dataset)</span>:</span></div><div class="line">    <span class="string">''' 对给定数据集进行回归并计算误差</span></div><div class="line">    '''</div><div class="line">    w, X, y = linear_regression(dataset)</div><div class="line">    y_prime = X*w</div><div class="line">    <span class="keyword">return</span> np.var(y_prime - y)</div></pre></td></tr></table></figure></p>
<h4 id="在分段线性数据上应用模型树"><a href="#在分段线性数据上应用模型树" class="headerlink" title="在分段线性数据上应用模型树"></a>在分段线性数据上应用模型树</h4><p>本部分使用了事先准备好的分段线性数据来构建模型树，数据点可视化如下:</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/exp2_data.png" alt=""></p>
<p>现在我们使用这些数据构建一个模型树:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tree = create_tree(dataset, fleaf, ferr, opt=&#123;<span class="string">'err_tolerance'</span>: <span class="number">0.1</span>, <span class="string">'n_tolerance'</span>: <span class="number">4</span>&#125;)</div><div class="line">tree</div></pre></td></tr></table></figure></p>
<p>得到的树结构：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="string">'feat_idx'</span>: <span class="number">0</span>,</div><div class="line"> <span class="string">'feat_val'</span>: <span class="number">0.30440099999999998</span>,</div><div class="line"> <span class="string">'left'</span>: matrix([[ <span class="number">3.46877936</span>],</div><div class="line">                 [ <span class="number">1.18521743</span>]]),</div><div class="line"> <span class="string">'right'</span>: matrix([[  <span class="number">1.69855694e-03</span>],</div><div class="line">                  [  <span class="number">1.19647739e+01</span>]])&#125;</div></pre></td></tr></table></figure></p>
<p>可视化:</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/exp2_tree.png" alt=""></p>
<p>绘制回归曲线:</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/exp2_regression.png" alt=""></p>
<p>可以通过模型树看到对于此数据只需要两个分支，数的深度也只有2层。</p>
<ul>
<li>当$x &lt; 0.304$的时候，使用线性模型$y = 3.47 + 1.19x$来回归</li>
<li>当$x &gt; 0.304$的时候，使用线性模型$y = 0.0017 + 1.20x$来回归</li>
</ul>
<h3 id="回归树与线性回归的对比"><a href="#回归树与线性回归的对比" class="headerlink" title="回归树与线性回归的对比"></a>回归树与线性回归的对比</h3><p>本部分我们使用标准线性回归和回归树分别对同一组数据进行回归，并使用同一组测试数据计算相关系数(Correlation Coefficient)对两种模型的回归效果进行对比。</p>
<p>数据我还是使用《Machinie Learning in Action》中的现成数据，数据可视化如下:</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/bike_data.png" alt=""></p>
<p>现在我们分别使用标准线性回归和回归树对该数据进行回归，并计算模型预测值和测试样本的相关系数$R^2$(完整代码见:<a href="https://github.com/PytLab/MLBox/blob/master/classification_and_regression_trees/compare.py" target="_blank" rel="noopener">https://github.com/PytLab/MLBox/blob/master/classification_and_regression_trees/compare.py</a>)</p>
<p>相关系数计算:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_corrcoef</span><span class="params">(X, Y)</span>:</span></div><div class="line">    <span class="comment"># X Y 的协方差</span></div><div class="line">    cov = np.mean(X*Y) - np.mean(X)*np.mean(Y)</div><div class="line">    <span class="keyword">return</span> cov/(np.var(X)*np.var(Y))**<span class="number">0.5</span></div></pre></td></tr></table></figure></p>
<p>获得的相关系数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">linear regression correlation coefficient: <span class="number">0.9434684235674773</span></div><div class="line">regression tree correlation coefficient: <span class="number">0.9780307932704089</span></div></pre></td></tr></table></figure></p>
<p>绘制线性回归和树回归的回归曲线(黄色会树回归曲线，红色会线性回归):</p>
<p><img src="/assets/images/blog_img/2017-11-03-机器学习算法实践-树回归/bike_regression.png" alt=""></p>
<p>可见树回归方法在预测复杂数据的时候会比简单的线性模型更有效。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文对决策树用于连续数值的回归预测进行了介绍，并实现了回归树, 剪枝和模型树以及相应的树结构输出可视化等。对于模型树也给予了相应的Python实现并针对分段线性数据进行了回归测试。最后并对回归树模型和简单的标准线性回归模型进行了对比。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>《Machine Learning in Action》</li>
<li><a href="http://blog.csdn.net/u014568921/article/details/45082197" target="_blank" rel="noopener">CART分类与回归树的原理与实现</a></li>
</ul>
	  
	</div>

    
	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
    	<li class="prev"><a href="/2018/01/24/实现属于自己的TensorFlow-一-计算图与前向传播/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/2017/10/27/机器学习实践-岭回归和LASSO回归/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>
    
	
    <!-- bdshare -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    

	<!-- comment -->
    
<section id="comment">
  <h2 class="title">Comments</h2>

  
  	 <div id="disqus_thread">
     <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  	 </div>
  
</section>

	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2017-11-03 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/学习小结/">学习小结<span class="badge">93</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/MachineLearning/">MachineLearning<span class="badge">16</span></a></li> <li><a href="/tags/LinearRegression/">LinearRegression<span class="badge">3</span></a></li> <li><a href="/tags/CART/">CART<span class="badge">1</span></a></li>

    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#前言"><span class="toc-article-text">前言</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#正文"><span class="toc-article-text">正文</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#CART"><span class="toc-article-text">CART</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#特征和最佳分割点的选取"><span class="toc-article-text">特征和最佳分割点的选取</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#树分裂的终止条件"><span class="toc-article-text">树分裂的终止条件</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#回归树的Python实现"><span class="toc-article-text">回归树的Python实现</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#使用回归树对数据进行回归"><span class="toc-article-text">使用回归树对数据进行回归</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#可视化数据点"><span class="toc-article-text">可视化数据点</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#创建回归树并可视化"><span class="toc-article-text">创建回归树并可视化</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#绘制回归树回归曲线"><span class="toc-article-text">绘制回归树回归曲线</span></a></li></ol></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#树剪枝"><span class="toc-article-text">树剪枝</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#预剪枝"><span class="toc-article-text">预剪枝</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#后剪枝"><span class="toc-article-text">后剪枝</span></a></li></ol></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#模型树"><span class="toc-article-text">模型树</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#创建模型树"><span class="toc-article-text">创建模型树</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#在分段线性数据上应用模型树"><span class="toc-article-text">在分段线性数据上应用模型树</span></a></li></ol></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#回归树与线性回归的对比"><span class="toc-article-text">回归树与线性回归的对比</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#总结"><span class="toc-article-text">总结</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#参考"><span class="toc-article-text">参考</span></a></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->

<script type="text/javascript">
var disqus_shortname = 'pytlab';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  
  &copy; copyright 2018 by <a href="http://pytlab.github.io"> PytLab </a>
  
      &nbsp; <a href="http://github.com/PytLab/hexo-theme-freemind/">Theme</a> by <a href="http://pytlab.github.io/">PytLab</a> based on <a href="https://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



</body>
   </html>
