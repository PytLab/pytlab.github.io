<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>机器学习算法实践-SVM中的SMO算法 | PytLab</title>
  <meta name="author" content="PytLab">
  
  <meta name="description" content="Personal Blog of ShaoZhengjiang">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="机器学习算法实践-SVM中的SMO算法"/>
  <meta property="og:site_name" content="PytLab"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <link rel="alternative" href="/true" title="PytLab" type="application/atom+xml">
  
  
    <link href="/assets/images/favicon/icon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/bootstrap.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-73223373-1', 'auto');
  ga('send', 'pageview');
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->




</head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/"></a>
      <div class="collapse navbar-collapse nav-menu">
		    <ul class="nav navbar-nav">
		      

          <!-- Categories -->
          
          <li>
            <a href="/" title="PytLab's Home" style="font-weight: normal; font-family: Calibri,Arial; font-size: 18px">
              <i class="fa fa-bank"></i>Home
            </a>
          </li>
          
		      

          <!-- Categories -->
          
          <!-- Archives -->
          <li class="dropdown">
            <a href="/archives" class="dropdown-toggle" data-toggle="dropdown" title="All the articles." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
            <i class="fa fa-archive"></i>Archives
            <b class="caret"></b>   
            </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/archives" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Archives</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/2017/09/01/机器学习算法实践-SVM中的SMO算法/" style="font-size: 15px; font-family: 微软雅黑">机器学习算法实践-SVM中的SMO算法<span></span></a></li>
              
              <li><a href="/2017/08/30/机器学习算法实践-SVM核函数和软间隔/" style="font-size: 15px; font-family: 微软雅黑">机器学习算法实践-SVM核函数和软间隔<span></span></a></li>
              
              <li><a href="/2017/08/15/机器学习算法实践-支持向量机-SVM-算法原理/" style="font-size: 15px; font-family: 微软雅黑">机器学习算法实践-支持向量机(SVM)算法原理<span></span></a></li>
              
              <li><a href="/2017/08/12/超过150个最好的与机器学习-自然语言处理和Python相关的教程/" style="font-size: 15px; font-family: 微软雅黑">150多个最好的与机器学习,自然语言处理和Python...<span></span></a></li>
              
              <li><a href="/2017/08/11/递归式求解-代入法/" style="font-size: 15px; font-family: 微软雅黑">递归式求解-代入法<span></span></a></li>
              
              <li><a href="/2017/08/02/使用MPI并行化遗传算法/" style="font-size: 15px; font-family: 微软雅黑">使用MPI并行化遗传算法框架GAFT<span></span></a></li>
              
              <li><a href="/2017/07/25/机器学习-Python-数学相关的速查表/" style="font-size: 15px; font-family: 微软雅黑">与机器学习, Python, 数学相关的速查表整理<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
          </li>

          
		      

          <!-- Categories -->
          
		      <li class="dropdown">
            <a href="/categories" class="dropdown-toggle" data-toggle="dropdown" title="All the categories." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
		    	  <i class="fa fa-folder"></i>Categories
            <b class="caret"></b>   
		    	  </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/categories" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Categories</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/categories/学习小结/" style="font-size: 15px; font-family: 微软雅黑">学习小结<span></span></a></li>
              
              <li><a href="/categories/学术/" style="font-size: 15px; font-family: 微软雅黑">学术<span></span></a></li>
              
              <li><a href="/categories/代码作品/" style="font-size: 15px; font-family: 微软雅黑">代码作品<span></span></a></li>
              
              <li><a href="/categories/教程/" style="font-size: 15px; font-family: 微软雅黑">教程<span></span></a></li>
              
              <li><a href="/categories/我的日常/" style="font-size: 15px; font-family: 微软雅黑">我的日常<span></span></a></li>
              
              <li><a href="/categories/译文/" style="font-size: 15px; font-family: 微软雅黑">译文<span></span></a></li>
              
              <li><a href="/categories/随笔/" style="font-size: 15px; font-family: 微软雅黑">随笔<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
		      </li>

          
		      

          <!-- Categories -->
          
          <!-- Tags -->
          <li class="dropdown">
            <a href="/tags" class="dropdown-toggle" data-toggle="dropdown" title="All the tags." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
            <i class="fa fa-tags"></i>Tags
            <b class="caret"></b>   
            </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/tags" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Tags</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/tags/Cpp/" style="font-size: 15px; font-family: 微软雅黑">Cpp<span></span></a></li>
              
              <li><a href="/tags/python/" style="font-size: 15px; font-family: 微软雅黑">python<span></span></a></li>
              
              <li><a href="/tags/catalysis/" style="font-size: 15px; font-family: 微软雅黑">catalysis<span></span></a></li>
              
              <li><a href="/tags/C/" style="font-size: 15px; font-family: 微软雅黑">C<span></span></a></li>
              
              <li><a href="/tags/chemistry/" style="font-size: 15px; font-family: 微软雅黑">chemistry<span></span></a></li>
              
              <li><a href="/tags/Parallel-Computing/" style="font-size: 15px; font-family: 微软雅黑">Parallel Computing<span></span></a></li>
              
              <li><a href="/tags/MPI/" style="font-size: 15px; font-family: 微软雅黑">MPI<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
          </li>
          
          
		      

          <!-- Categories -->
          
          <li>
            <a href="/about" title="About me." style="font-weight: normal; font-family: Calibri,Arial; font-size: 18px">
              <i class="fa fa-user"></i>About
            </a>
          </li>
          
		      
		    </ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">		
			<h1> 机器学习算法实践-SVM中的SMO算法</h1>
		</div>		
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>前两篇关于SVM的文章分别总结了SVM基本原理和核函数以及软间隔原理，本文我们就针对前面推导出的SVM对偶问题的一种高效的优化方法-序列最小优化算法(Sequential Minimal Optimization, SMO)的原理进行总结并进行相应的Python实现。</p>
<h2 id="坐标上升算法-Coordiante-Ascent"><a href="#坐标上升算法-Coordiante-Ascent" class="headerlink" title="坐标上升算法(Coordiante Ascent)"></a>坐标上升算法(Coordiante Ascent)</h2><p>在SMO算法之前，还是需要总结下坐标上升算法，因为SMO算法的思想与坐标上升算法的思想类似。</p>
<p>坐标上升算法每次通过更新多元函数中的一维，经过多次迭代直到收敛来达到优化函数的目的。简单的讲就是不断地选中一个变量做一维最优化直到函数达到局部最优点。</p>
<a id="more"></a>
<p>假设我们需要求解的问题形式为(类似我们SVM的对偶形式):<br>$$\max \limits_{\alpha} W(\alpha_{1}, \alpha_{2}, …, \alpha_{N})$$</p>
<h3 id="算法过程伪码"><a href="#算法过程伪码" class="headerlink" title="算法过程伪码:"></a>算法过程伪码:</h3><p><img src="/assets/images/blog_img/2017-09-01-机器学习算法实践-SVM中的SMO算法/coord_ascent_algo.png" alt=""></p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>若我们的优化目标为一个二元函数:<br>$$<br>arg \min \limits_{x_{1}, x_{2}} f(x_{1}, x_{2}) = -x_{1}^{2} - 3x_{2}^{2} + 2x_{1}x_{2} + 6<br>$$</p>
<p>我们先给一个$(x_{1}, x_{2})$的初值然后开始迭代。</p>
<ol>
<li><p>先固定$x_{1}$，把$f$看做$x_{2}$的一元函数求最优值，可以简单的进行求导获取解析解:<br>$$<br>\frac{\partial{f}}{\partial{x_{1}}} = -2x_{1} + 2x_{2} = 0 \rightarrow x_{1} = x_{2}<br>$$</p>
</li>
<li><p>在固定$x_{2}$, 把$f$看成$x_{1}$的一元函数求最优值，得到$x_{1}$的解析解:<br>$$<br>\frac{\partial{f}}{\partial{x_{2}}} = -6x_{2} + 2x_{1} \rightarrow x_{2} = \frac{1}{3}x_{1}<br>$$</p>
</li>
</ol>
<p>安装上面两个过程不断交替的优化$x_{1}$和$x_{2}$，直到函数收敛。</p>
<p>通过下面的图就可以看出，优化的过程，因为每次只优化一个变量，每次迭代的方向都是沿着坐标轴方向的。</p>
<p><img src="/assets/images/blog_img/2017-09-01-机器学习算法实践-SVM中的SMO算法/coord_ascent_pic.png" alt=""></p>
<p>因为每次只是做一维优化，所以每个循环中的优化过程的效率是很高的, 但是迭代的次数会比较多。</p>
<h2 id="序列最小优化算法-SMO"><a href="#序列最小优化算法-SMO" class="headerlink" title="序列最小优化算法(SMO)"></a>序列最小优化算法(SMO)</h2><h3 id="SMO算法介绍"><a href="#SMO算法介绍" class="headerlink" title="SMO算法介绍"></a>SMO算法介绍</h3><p>SMO的思想类似坐标上升算法，我们需要优化一系列的$\alpha$的值，我们每次选择尽量少的$\alpha$来优化，不断迭代直到函数收敛到最优值。</p>
<p>来到SVM的对偶问题上，对偶形式:<br>$$arg \max \limits_{\alpha} \sum_{i=1}^{N} \alpha_{i} - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}y_{i}y_{j}\alpha_{i}\alpha_{j}\langle x_{i}, x_{j} \rangle$$</p>
<p>subject to $\alpha_{i} \ge 0$ ; $\sum_{i=1}^{N}\alpha_{i}y_{i}=0$</p>
<p>其中我们需要对$(\alpha_{1}, \alpha_{2}, …, \alpha_{N})$进行优化，但是这个凸二次优化问题的其他求解算法的复杂度很高，但是Platt提出的SMO算法可以高效的求解上述对偶问题，他把原始问题的求解$N$个参数二次规划问题分解成多个二次规划问题求解，每个字问题只需要求解2各参数，节省了时间成本和内存需求。</p>
<p>与坐标上升算法不同的是，我们在SMO算法中我们每次需要选择<strong>一对</strong>变量$(\alpha_{i}, \alpha_{j})$, 因为在SVM中，我们的$\alpha$并不是完全独立的，而是具有约束的:<br>$$\sum_{i=1}^{N}\alpha_{i}y_{i} = 0$$</p>
<p>因此一个$\alpha$改变，另一个也要随之变化以满足条件。</p>
<h3 id="SMO算法原理"><a href="#SMO算法原理" class="headerlink" title="SMO算法原理"></a>SMO算法原理</h3><h4 id="获得没有修剪的原始解"><a href="#获得没有修剪的原始解" class="headerlink" title="获得没有修剪的原始解"></a>获得没有修剪的原始解</h4><p>假设我们选取的两个需要优化的参数为$\alpha_{1}, \alpha_{2}$, 剩下的$\alpha_{3}, \alpha_{4}, …, \alpha_{N}$则固定，作为常数处理。将SVM优化问题进行展开就可以得到(把与$\alpha_{1}, \alpha_{2}$无关的项合并成常数项$C$): </p>
<p>$$<br>W(\alpha_{1}, \alpha_{2}) = \alpha_{1} + \alpha_{2} - \frac{1}{2}K_{1,1}y_{1}^{2}\alpha_{1}^{2} - \frac{1}{2}K_{2,2}y_{2}^{2}\alpha_{2}^{2} - K_{1,2}y_{1}y_{2}\alpha_{1}\alpha_{2} - y_{1}\alpha_{1}\sum_{i=3}^{N}\alpha_{i}y_{i}K_{i,1} - y_{2}\alpha_{2}\sum_{i=3}^{N}\alpha_{i}y_{i}K_{i, 2} + C<br>$$</p>
<p>于是就是一个二元函数的优化:</p>
<p>$$<br>arg \max \limits_{\alpha_{1}, \alpha_{2}} W(\alpha_{1}, \alpha_{2})<br>$$</p>
<p>根据约束条件$\sum_{i=1}^{N}\alpha_{i}y_{i} = 0$可以得到$\alpha_{1}与\alpha_{2}$的关系:</p>
<p>$$<br>\alpha_{1}y_{1} + \alpha_{2}y_{2} = -\sum_{i=3}^{N}\alpha_{i}y_{i} = \zeta<br>$$</p>
<p>两边同时乘上$y_{1}$, 由于$y_{i}y_{i} = 1$得到:</p>
<p>$$\alpha_{1} = \zeta y_{1} - \alpha_{2}y_{1}y_{2}$$</p>
<p>令$v_{1} = \sum_{i=3}^{N}\alpha_{i}y_{i}K_{i, 1}$, $v_{2} = \sum_{i=3}^{N}\alpha_{i}y_{i}K_{i, 2}$，$\alpha_{1}$的表达式代入得到:</p>
<p>$$<br>W(\alpha_{2}) = -\frac{1}{2}K_{1, 1}(\zeta - \alpha_{2}y_{2})^{2} - \frac{1}{2}K_{2, 2}\alpha_{2}^{2} - y_{2}(\zeta - \alpha_{2}y_{2})\alpha_{2}K_{1, 2} - v_{1}(\zeta - \alpha_{2}y_{2}) - v_{2}y_{2}\alpha_{2} + \alpha_{1} + \alpha_{2} + C<br>$$</p>
<p>后面我们需要对这个一元函数进行求极值，$W$对$\alpha$的一阶导数为0得到:</p>
<p>$$<br>\frac{\partial{W(\alpha_{2})}}{\partial{\alpha_{2}}} = -(K_{1, 1} + K_{2, 2} - 2K_{1, 2})\alpha_{2} + K_{1, 1}\zeta y_{2} - K_{1, 2}\zeta y_{2} + v_{1}y_{2} - v_{2}y_{2} - y_{1}y_{2} + y_{2}^{2} = 0<br>$$</p>
<p>下面我们稍微对上式进行下变形，使得$\alpha_{2}^{new}$能够用更新前的$\alpha_{2}^{old}$表示，而不是使用不方便计算的$\zeta$。</p>
<p>因为SVM对数据点的预测值为: $f(x) = \sum_{i=1}^{N}\alpha_{i}y_{i} K(x_{i}, x) + b$</p>
<p>则$v_{1}$以及$v_{2}$的值可以表示成:</p>
<p>$$<br>v_{1} = \sum_{i=3}^{N}\alpha_{i}y_{i}K_{1, i} = f(x_{1}) - \alpha_{1}y_{1}K_{1, 1} - \alpha_{2}y_{2}K_{1, 2} - b<br>$$</p>
<p>$$<br>v_{2} = \sum_{i=3}^{N}\alpha_{i}y_{i}K_{2, i} = f(x_{2}) - \alpha_{1}y_{1}K_{1, 2} - \alpha_{2}y_{2}K_{2, 2} - b<br>$$</p>
<p>已知$\alpha_{1} = (\zeta - \alpha_{2}y_{2})y_{2}$, 可得到:<br>$$<br>v_{1} - v_{2} = f(x_{1}) - f(x_{2}) - K_{1, 1}\zeta + K_{1, 2}\zeta + (K_{1, 1} + K_{2, 2} - 2K_{1, 2})\alpha_{2}y_{2}<br>$$</p>
<p>将$v_{1} - v_{2}$的表达式代入到$\frac{\partial{W(\alpha_{2})}}{\partial{\alpha_{2}}}$中可以得到:</p>
<p>$$<br>\frac{\partial{W(\alpha_{2})}}{\partial{\alpha_{2}}} = -(K_{1, 1}) + K_{2, 2} - 2K_{1, 2})\alpha_{2}^{new} +(K_{1, 1}) + K_{2, 2} - 2K_{1, 2})\alpha_{2}^{old} + y_{2}\left[ y_{2} - y_{1} + f(x_{1}) - f(x_{2}) \right]<br>$$</p>
<p>我们记$E_{i}$为SVM预测值与真实值的误差: $E_{i} = f(x_{i}) - y_{i}$</p>
<p>令$\eta = K_{1, 1} + K_{2, 2} - 2K_{1, 2}$得到最终的一阶导数表达式:</p>
<p>$$<br>\frac{\partial{W(\alpha_{2})}}{\partial{\alpha_{2}}} = -\eta \alpha_{2}^{new} + \eta \alpha_{2}^{old} + y_{2}(E_{1} - E_{2}) = 0<br>$$</p>
<p>得到:<br>$$<br>\alpha_{2}^{new} = \alpha_{2}^{old} + \frac{y_{2}(E_{1} - E_{2})}{\eta}<br>$$</p>
<p>这样我们就得到了通过旧的$\alpha_{2}$获取新的$\alpha_{2}$的表达式, $\alpha_{1}^{new}$便可以通过$\alpha_{2}^{new}$得到。</p>
<h4 id="对原始解进行修剪"><a href="#对原始解进行修剪" class="headerlink" title="对原始解进行修剪"></a>对原始解进行修剪</h4><p>上面我们通过对一元函数求极值的方式得到的最优$\alpha_{i}, \alpha_{j}$是未考虑约束条件下的最优解，我们便更正我们上部分得到的$\alpha_{2}^{new}$为$\alpha_{2}^{new, unclipped}$, 即:<br>$$<br>\alpha_{2}^{new, unclipped} = \alpha_{2}^{old} + \frac{y_{2}(E_{1} - E_{2})}{\eta}<br>$$</p>
<p>但是在SVM中我们的$\alpha_{i}$是有约束的，即:<br>$\alpha_{1}y_{1} + \alpha_{2}y_{2} = -\sum_{i=3}^{N}\alpha_{i}y_{i} = \zeta$<br>$0 \le \alpha_{i} \le C$</p>
<p>此约束为方形约束(Bosk constraint), 在二维平面中我们可以看到这是个限制在方形区域中的直线（见下图）。</p>
<p><img src="/assets/images/blog_img/2017-09-01-机器学习算法实践-SVM中的SMO算法/bosk_constraint.png" alt=""></p>
<ol>
<li><p>(如左图) 当$y_{1} \ne y_{2}$时，线性限制条件可以写成: $\alpha_{1} - \alpha_{2} = k$，根据$k$的正负可以得到不同的上下界，因此统一表示成:</p>
<ul>
<li>下界: $L = \max(0, \alpha_{2}^{old} - \alpha_{1}^{old})$</li>
<li>上界: $H = \min(C, C + \alpha_{2}^{old} - \alpha_{1}^{old})$</li>
</ul>
</li>
<li><p>(如右图) 当$y_{1} = y_{2}$时，限制条件可写成: $\alpha_{1} + \alpha_{2} = k$, 上下界表示成:</p>
<ul>
<li>下界: $L = \max(0, \alpha_{1}^{old} + \alpha_{2}^{old} - C)$</li>
<li>上界: $H = \min(C, \alpha_{2}^{old} + \alpha_{1}^{old})$</li>
</ul>
</li>
</ol>
<p>根据得到的上下界，我们可以得到修剪后的$\alpha_{2}^{new}$:<br>$$<br>\alpha_{2}^{new} =<br>\begin{cases}<br>H &amp; \alpha_{2}^{new, unclipped} &gt; H \\<br>\alpha_{2}^{new, unclipped} &amp; L \le \alpha_{2}^{new, unclipped} \le H \\<br>L &amp; \alpha_{2}^{new, unclipped} &lt; L<br>\end{cases}<br>$$</p>
<p>得到了$\alpha_{2}^{new}$我们便可以根据$\alpha_{1}^{old}y_{1} + \alpha_{2}^{old}y_{2} = \alpha_{1}^{new}y_{1} + \alpha_{2}^{new}y_{2}$得到$\alpha_{1}^{new}$:<br>$$<br>\alpha_{1}^{new} = \alpha_{1}^{old} + y_{1}y_{2}(\alpha_{2}^{old} - \alpha_{2}^{new})<br>$$</p>
<p>OK， 这样我们就知道如何将选取的一对$\alpha_{i}, \alpha_{j}$进行优化更新了。</p>
<h4 id="更新阈值-b"><a href="#更新阈值-b" class="headerlink" title="更新阈值$b$"></a>更新阈值$b$</h4><p>当我们更新了一对$\alpha_{i}, \alpha_{j}$之后都需要重新计算阈值$b$，因为$b$关系到我们$f(x)$的计算，关系到下次优化的时候误差$E_{i}$的计算。</p>
<p>为了使得被优化的样本都满足KKT条件，</p>
<p>当$\alpha_{1}^{new}$不在边界，即$0 &lt; \alpha_{1}^{new} &lt; C$, 根据KKT条件可知相应的数据点为支持向量，满足$y_{1}(w^{T} + b) = 1$, 两边同时乘上$y_{1}$得到$\sum_{i=1}^{N}\alpha_{i}y_{i}K_{i, 1} + b = y_{1}$, 进而得到$b_{1}^{new}$的值:<br>$$<br>b_{1}^{new} = y_{1} - \sum_{i=3}^{N}\alpha_{i}y_{i}K_{i, 1} - \alpha_{1}^{new}y_{1}K_{1, 1} - \alpha_{2}^{new}y_{2}K_{2, 1}<br>$$</p>
<p>其中上式的前两项可以写成:<br>$$<br>y_{1} - \sum_{i=3}^{N}\alpha_{i}y_{i}K_{i, 1} = -E_{1} + \alpha_{1}^{old}y_{1}K_{1, 1} + \alpha_{2}^{old}y_{2}K_{2, 1} + b^{old}<br>$$</p>
<p>代入$b_{new}$的表达式中得到:<br>$$<br>b_{1}^{new} = -E_{1} - y_{1}K_{1, 1}(\alpha_{1}^{new} - \alpha_{1}^{old}) - y_{2}K_{2, 1}(\alpha_{2}^{new} - \alpha_{2}^{old}) + b^{old}<br>$$</p>
<p>当$0 &lt; \alpha_{2}^{new} &lt; C$, 可以得到$b_{2}^{new}$的表达式(推导同上):</p>
<p>$$<br>b_{2}^{new} = -E_{2} - y_{1}K_{1, 2}(\alpha_{1}^{new} - \alpha_{1}^{old}) - y_{2}K_{2, 2}(\alpha_{2}^{new} - \alpha_{2}^{old}) + b^{old}<br>$$</p>
<p>当$b_{1}$和$b_{2}$都有效的时候他们是相等的, 即$b^{new} = b_{1}^{new} = b_{2}^{new}$。</p>
<p>当两个乘子$\alpha_{1}, \alpha_{2}$都在边界上，且$L \ne H$时，$b1, b2$之间的值就是和KKT条件一直的阈值。SMO选择他们的中点作为新的阈值: </p>
<p>$$b^{new} = \frac{b_{1}^{new} + b_{2}^{new}}{2}$$</p>
<h2 id="简化版SMO算法实现"><a href="#简化版SMO算法实现" class="headerlink" title="简化版SMO算法实现"></a>简化版SMO算法实现</h2><p>这里我主要针对SMO中已选取的一对$\alpha_{i}, \alpha_{j}$值的优化过程进行下Python实现，其中$\alpha_{i}, \alpha_{j}$的选取直接使用傻瓜的遍历方式，并使用100数据点进行训练。</p>
<p>首先是一些辅助函数，用来帮助加载数据，修剪$\alpha$的值以及随机选取$\alpha$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(filename)</span>:</span></div><div class="line">    dataset, labels = [], []</div><div class="line">    <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</div><div class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</div><div class="line">            x, y, label = [float(i) <span class="keyword">for</span> i <span class="keyword">in</span> line.strip().split()]</div><div class="line">            dataset.append([x, y])</div><div class="line">            labels.append(label)</div><div class="line">    <span class="keyword">return</span> dataset, labels</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(alpha, L, H)</span>:</span></div><div class="line">    <span class="string">''' 修建alpha的值到L和H之间.</span></div><div class="line">    '''</div><div class="line">    <span class="keyword">if</span> alpha &lt; L:</div><div class="line">        <span class="keyword">return</span> L</div><div class="line">    <span class="keyword">elif</span> alpha &gt; H:</div><div class="line">        <span class="keyword">return</span> H</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> alpha</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_j</span><span class="params">(i, m)</span>:</span></div><div class="line">    <span class="string">''' 在m中随机选择除了i之外剩余的数</span></div><div class="line">    '''</div><div class="line">    l = list(range(m))</div><div class="line">    seq = l[: i] + l[i+<span class="number">1</span>:]</div><div class="line">    <span class="keyword">return</span> random.choice(seq)</div></pre></td></tr></table></figure>
<p>为了能在最后绘制SVM分割线，我们需要根据获取的$\alpha$，数据点以及标签来获取$w$的值:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_w</span><span class="params">(alphas, dataset, labels)</span>:</span></div><div class="line">    <span class="string">''' 通过已知数据点和拉格朗日乘子获得分割超平面参数w</span></div><div class="line">    '''</div><div class="line">    alphas, dataset, labels = np.array(alphas), np.array(dataset), np.array(labels)</div><div class="line">    yx = labels.reshape(<span class="number">1</span>, <span class="number">-1</span>).T*np.array([<span class="number">1</span>, <span class="number">1</span>])*dataset</div><div class="line">    w = np.dot(yx.T, alphas)</div><div class="line"></div><div class="line">    <span class="keyword">return</span> w.tolist()</div></pre></td></tr></table></figure></p>
<p>简化版SMO算法的实现,即便没有添加启发式的$\alpha$选取，SMO算法仍然有比较多的公式需要实现，我本人按照上文的推导进行实现的时候就因为写错了一个下标算法一直跑不出想要的结果。</p>
<p>此实现主要包含两重循环，外层循环是控制最大迭代步数，此迭代步数是在每次有优化一对$\alpha$之后进行判断所选取的$\alpha$是否已被优化，如果没有则进行加一，如果连续$max_iter$步数之后仍然没有$\alpha$被优化，则我们就认为所有的$\alpha$基本已经被优化，优化便可以终止了.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_smo</span><span class="params">(dataset, labels, C, max_iter)</span>:</span></div><div class="line">    <span class="string">''' 简化版SMO算法实现，未使用启发式方法对alpha对进行选择.</span></div><div class="line"></div><div class="line">    :param dataset: 所有特征数据向量</div><div class="line">    :param labels: 所有的数据标签</div><div class="line">    :param C: 软间隔常数, 0 &lt;= alpha_i &lt;= C</div><div class="line">    :param max_iter: 外层循环最大迭代次数</div><div class="line">    '''</div><div class="line">    dataset = np.array(dataset)</div><div class="line">    m, n = dataset.shape</div><div class="line">    labels = np.array(labels)</div><div class="line"></div><div class="line">    <span class="comment"># 初始化参数</span></div><div class="line">    alphas = np.zeros(m)</div><div class="line">    b = <span class="number">0</span></div><div class="line">    it = <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></div><div class="line">        <span class="string">"SVM分类器函数 y = w^Tx + b"</span></div><div class="line">        <span class="comment"># Kernel function vector.</span></div><div class="line">        x = np.matrix(x).T</div><div class="line">        data = np.matrix(dataset)</div><div class="line">        ks = data*x</div><div class="line"></div><div class="line">        <span class="comment"># Predictive value.</span></div><div class="line">        wx = np.matrix(alphas*labels)*ks</div><div class="line">        fx = wx + b</div><div class="line"></div><div class="line">        <span class="keyword">return</span> fx[<span class="number">0</span>, <span class="number">0</span>]</div><div class="line"></div><div class="line">    all_alphas, all_bs = [], []</div><div class="line"></div><div class="line">    <span class="keyword">while</span> it &lt; max_iter:</div><div class="line">        pair_changed = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</div><div class="line">            a_i, x_i, y_i = alphas[i], dataset[i], labels[i]</div><div class="line">            fx_i = f(x_i)</div><div class="line">            E_i = fx_i - y_i</div><div class="line"></div><div class="line">            j = select_j(i, m)</div><div class="line">            a_j, x_j, y_j = alphas[j], dataset[j], labels[j]</div><div class="line">            fx_j = f(x_j)</div><div class="line">            E_j = fx_j - y_j</div><div class="line"></div><div class="line">            K_ii, K_jj, K_ij = np.dot(x_i, x_i), np.dot(x_j, x_j), np.dot(x_i, x_j)</div><div class="line">            eta = K_ii + K_jj - <span class="number">2</span>*K_ij</div><div class="line">            <span class="keyword">if</span> eta &lt;= <span class="number">0</span>:</div><div class="line">                print(<span class="string">'WARNING  eta &lt;= 0'</span>)</div><div class="line">                <span class="keyword">continue</span></div><div class="line"></div><div class="line">            <span class="comment"># 获取更新的alpha对</span></div><div class="line">            a_i_old, a_j_old = a_i, a_j</div><div class="line">            a_j_new = a_j_old + y_j*(E_i - E_j)/eta</div><div class="line"></div><div class="line">            <span class="comment"># 对alpha进行修剪</span></div><div class="line">            <span class="keyword">if</span> y_i != y_j:</div><div class="line">                L = max(<span class="number">0</span>, a_j_old - a_i_old)</div><div class="line">                H = min(C, C + a_j_old - a_i_old)</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                L = max(<span class="number">0</span>, a_i_old + a_j_old - C)</div><div class="line">                H = min(C, a_j_old + a_i_old)</div><div class="line"></div><div class="line">            a_j_new = clip(a_j_new, L, H)</div><div class="line">            a_i_new = a_i_old + y_i*y_j*(a_j_old - a_j_new)</div><div class="line"></div><div class="line">            <span class="keyword">if</span> abs(a_j_new - a_j_old) &lt; <span class="number">0.00001</span>:</div><div class="line">                <span class="comment">#print('WARNING   alpha_j not moving enough')</span></div><div class="line">                <span class="keyword">continue</span></div><div class="line"></div><div class="line">            alphas[i], alphas[j] = a_i_new, a_j_new</div><div class="line"></div><div class="line">            <span class="comment"># 更新阈值b</span></div><div class="line">            b_i = -E_i - y_i*K_ii*(a_i_new - a_i_old) - y_j*K_ij*(a_j_new - a_j_old) + b</div><div class="line">            b_j = -E_j - y_i*K_ij*(a_i_new - a_i_old) - y_j*K_jj*(a_j_new - a_j_old) + b</div><div class="line"></div><div class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt; a_i_new &lt; C:</div><div class="line">                b = b_i</div><div class="line">            <span class="keyword">elif</span> <span class="number">0</span> &lt; a_j_new &lt; C:</div><div class="line">                b = b_j</div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                b = (b_i + b_j)/<span class="number">2</span></div><div class="line"></div><div class="line">            all_alphas.append(alphas)</div><div class="line">            all_bs.append(b)</div><div class="line"></div><div class="line">            pair_changed += <span class="number">1</span></div><div class="line">            print(<span class="string">'INFO   iteration:&#123;&#125;  i:&#123;&#125;  pair_changed:&#123;&#125;'</span>.format(it, i, pair_changed))</div><div class="line"></div><div class="line">        <span class="keyword">if</span> pair_changed == <span class="number">0</span>:</div><div class="line">            it += <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            it = <span class="number">0</span></div><div class="line">        print(<span class="string">'iteration number: &#123;&#125;'</span>.format(it))</div><div class="line"></div><div class="line">    <span class="keyword">return</span> alphas, b</div></pre></td></tr></table></figure>
<p>Ok, 下面我们就用训练数据对SVM进行优化, 并对最后优化的分割线以及数据点进行可视化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> <span class="string">'__main__'</span> == __name__:</div><div class="line">    <span class="comment"># 加载训练数据</span></div><div class="line">    dataset, labels = load_data(<span class="string">'testSet.txt'</span>)</div><div class="line">    <span class="comment"># 使用简化版SMO算法优化SVM</span></div><div class="line">    alphas, b = simple_smo(dataset, labels, <span class="number">0.6</span>, <span class="number">40</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 分类数据点</span></div><div class="line">    classified_pts = &#123;<span class="string">'+1'</span>: [], <span class="string">'-1'</span>: []&#125;</div><div class="line">    <span class="keyword">for</span> point, label <span class="keyword">in</span> zip(dataset, labels):</div><div class="line">        <span class="keyword">if</span> label == <span class="number">1.0</span>:</div><div class="line">            classified_pts[<span class="string">'+1'</span>].append(point)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            classified_pts[<span class="string">'-1'</span>].append(point)</div><div class="line"></div><div class="line">    fig = plt.figure()</div><div class="line">    ax = fig.add_subplot(<span class="number">111</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 绘制数据点</span></div><div class="line">    <span class="keyword">for</span> label, pts <span class="keyword">in</span> classified_pts.items():</div><div class="line">        pts = np.array(pts)</div><div class="line">        ax.scatter(pts[:, <span class="number">0</span>], pts[:, <span class="number">1</span>], label=label)</div><div class="line"></div><div class="line">    <span class="comment"># 绘制分割线</span></div><div class="line">    w = get_w(alphas, dataset, labels)</div><div class="line">    x1, _ = max(dataset, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</div><div class="line">    x2, _ = min(dataset, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</div><div class="line">    a1, a2 = w</div><div class="line">    y1, y2 = (-b - a1*x1)/a2, (-b - a1*x2)/a2</div><div class="line">    ax.plot([x1, x2], [y1, y2])</div><div class="line"></div><div class="line">    <span class="comment"># 绘制支持向量</span></div><div class="line">    <span class="keyword">for</span> i, alpha <span class="keyword">in</span> enumerate(alphas):</div><div class="line">        <span class="keyword">if</span> abs(alpha) &gt; <span class="number">1e-3</span>:</div><div class="line">            x, y = dataset[i]</div><div class="line">            ax.scatter([x], [y], s=<span class="number">150</span>, c=<span class="string">'none'</span>, alpha=<span class="number">0.7</span>,</div><div class="line">                       linewidth=<span class="number">1.5</span>, edgecolor=<span class="string">'#AB3319'</span>)</div><div class="line"></div><div class="line">    plt.show()</div></pre></td></tr></table></figure>
<p>优化最后我们可以看到针对100个数据的$\alpha$只有少部分是大于零的，即对应的数据点就是支持向量:</p>
<p><img src="/assets/images/blog_img/2017-09-01-机器学习算法实践-SVM中的SMO算法/alphas.png" alt=""></p>
<p>为了能直观的显示支持向量，我将其标注了出来，最终可视化的效果如下图:</p>
<p><img src="/assets/images/blog_img/2017-09-01-机器学习算法实践-SVM中的SMO算法/simple_smo.png" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文从坐标上升算法开始介绍，并对SMO算法的原理进行了简单的推导，针对SMO算法中对$\alpha$对的优化并使用了Python进行了简化版的SMO实现，并针对小数据集进行了训练得到了对应优化后的SVM。</p>
<p>实现代码以及训练数据链接: <a href="https://github.com/PytLab/MLBox/tree/master/svm" target="_blank" rel="external">https://github.com/PytLab/MLBox/tree/master/svm</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a href="http://blog.csdn.net/google19890102/article/details/51065297" target="_blank" rel="external">优化算法——坐标上升法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28299882" target="_blank" rel="external">支持向量机系列（5）——SMO算法解对偶问题</a></li>
<li>《Machine Learning in Action》</li>
</ol>
	  
	</div>

    
	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
		
          <li class="prev disabled"><a><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
        

        <li><a href="/"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/2017/08/30/机器学习算法实践-SVM核函数和软间隔/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>
    
	
    <!-- bdshare -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    

	<!-- comment -->
    
<section id="comment">
  <h2 class="title">Comments</h2>

  
  	 <div id="disqus_thread">
     <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  	 </div>
  
</section>

	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2017-09-01 
	</div>
	

	<!-- categories -->
    

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/MachineLearning/">MachineLearning<span class="badge">9</span></a></li> <li><a href="/tags/SVM/">SVM<span class="badge">3</span></a></li> <li><a href="/tags/SMO/">SMO<span class="badge">1</span></a></li> <li><a href="/tags/CoordianteAscent/">CoordianteAscent<span class="badge">1</span></a></li>

    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#前言"><span class="toc-article-text">前言</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#坐标上升算法-Coordiante-Ascent"><span class="toc-article-text">坐标上升算法(Coordiante Ascent)</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#算法过程伪码"><span class="toc-article-text">算法过程伪码:</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#例子"><span class="toc-article-text">例子</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#序列最小优化算法-SMO"><span class="toc-article-text">序列最小优化算法(SMO)</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#SMO算法介绍"><span class="toc-article-text">SMO算法介绍</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#SMO算法原理"><span class="toc-article-text">SMO算法原理</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#获得没有修剪的原始解"><span class="toc-article-text">获得没有修剪的原始解</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#对原始解进行修剪"><span class="toc-article-text">对原始解进行修剪</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#更新阈值-b"><span class="toc-article-text">更新阈值$b$</span></a></li></ol></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#简化版SMO算法实现"><span class="toc-article-text">简化版SMO算法实现</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#总结"><span class="toc-article-text">总结</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#参考"><span class="toc-article-text">参考</span></a></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->

<script type="text/javascript">
var disqus_shortname = 'pytlab';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  
  &copy; copyright 2017 by <a href="http://pytlab.github.io"> PytLab </a>
  
      &nbsp; <a href="http://github.com/PytLab/hexo-theme-freemind/">Theme</a> by <a href="http://pytlab.github.io/">PytLab</a> based on <a href="https://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



</body>
   </html>
