<!DOCTYPE HTML>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>实现属于自己的TensorFlow(三) - 反向传播与梯度下降实现 | PytLab</title>
  <meta name="author" content="PytLab">
  
  <meta name="description" content="Personal Blog of ShaoZhengjiang">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="实现属于自己的TensorFlow(三) - 反向传播与梯度下降实现"/>
  <meta property="og:site_name" content="PytLab"/>

  
    <meta property="og:image" content="undefined"/>
  

  
    <link rel="alternative" href="/true" title="PytLab" type="application/atom+xml">
  
  
    <link href="/assets/images/favicon/icon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/bootstrap.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-73223373-1', 'auto');
  ga('send', 'pageview');
</script>




<meta name="generator" content="Hexo 5.0.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

 <body>  
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav id="main-nav" class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/"></a>
      <div class="collapse navbar-collapse nav-menu">
		    <ul class="nav navbar-nav">
		      

          <!-- Categories -->
          
          <li>
            <a href="/" title="PytLab's Home" style="font-weight: normal; font-family: Calibri,Arial; font-size: 18px">
              <i class="fa fa-home"></i>Home
            </a>
          </li>
          
		      

          <!-- Categories -->
          
          <!-- Archives -->
          <li class="dropdown">
            <a href="/archives" class="dropdown-toggle" data-toggle="dropdown" title="All the articles." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
            <i class="fa fa-archive"></i>Archives
            <b class="caret"></b>   
            </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/archives" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Archives</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/2020/08/09/Life-of-a-thread/" style="font-size: 15px; font-family: 微软雅黑">Life of a thread<span></span></a></li>
              
              <li><a href="/2020/03/08/A-new-start/" style="font-size: 15px; font-family: 微软雅黑">I&#39;m back<span></span></a></li>
              
              <li><a href="/2018/03/07/遗传算法框架GAFT支持自定义个体编码方式/" style="font-size: 15px; font-family: 微软雅黑">遗传算法框架GAFT已支持自定义编码方式<span></span></a></li>
              
              <li><a href="/2018/01/27/实现属于自己的TensorFlow-三-反向传播与梯度下降算法实现/" style="font-size: 15px; font-family: 微软雅黑">实现属于自己的TensorFlow(三) - 反向传播...<span></span></a></li>
              
              <li><a href="/2018/01/25/实现属于自己的TensorFlow-二-梯度计算与反向传播/" style="font-size: 15px; font-family: 微软雅黑">实现属于自己的TensorFlow(二) - 梯度计算...<span></span></a></li>
              
              <li><a href="/2018/01/24/实现属于自己的TensorFlow-一-计算图与前向传播/" style="font-size: 15px; font-family: 微软雅黑">实现属于自己的TensorFlow(一) - 计算图与...<span></span></a></li>
              
              <li><a href="/2017/11/03/机器学习算法实践-树回归/" style="font-size: 15px; font-family: 微软雅黑">机器学习算法实践-树回归<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
          </li>

          
		      

          <!-- Categories -->
          
		      <li class="dropdown">
            <a href="/categories" class="dropdown-toggle" data-toggle="dropdown" title="All the categories." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
		    	  <i class="fa fa-folder"></i>Categories
            <b class="caret"></b>   
		    	  </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/categories" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Categories</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/categories/学习小结/" style="font-size: 15px; font-family: 微软雅黑">学习小结<span></span></a></li>
              
              <li><a href="/categories/学术/" style="font-size: 15px; font-family: 微软雅黑">学术<span></span></a></li>
              
              <li><a href="/categories/代码作品/" style="font-size: 15px; font-family: 微软雅黑">代码作品<span></span></a></li>
              
              <li><a href="/categories/我的日常/" style="font-size: 15px; font-family: 微软雅黑">我的日常<span></span></a></li>
              
              <li><a href="/categories/教程/" style="font-size: 15px; font-family: 微软雅黑">教程<span></span></a></li>
              
              <li><a href="/categories/译文/" style="font-size: 15px; font-family: 微软雅黑">译文<span></span></a></li>
              
              <li><a href="/categories/随笔/" style="font-size: 15px; font-family: 微软雅黑">随笔<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
		      </li>

          
		      

          <!-- Categories -->
          
          <!-- Tags -->
          <li class="dropdown">
            <a href="/tags" class="dropdown-toggle" data-toggle="dropdown" title="All the tags." style="font-weight: normal; font-family: Calibri,Arial; font-size:     18px">
            <i class="fa fa-tags"></i>Tags
            <b class="caret"></b>   
            </a>
            <ul class="dropdown-menu">
              <li class="divider"></li>
              <li><a href="/tags" style="font-size: 20px; font-family: 'Calibri Light',Arial">All Tags</a><span></span></li>
              <li class="divider"></li>
              
              <li><a href="/tags/python/" style="font-size: 15px; font-family: 微软雅黑">python<span></span></a></li>
              
              <li><a href="/tags/Cpp/" style="font-size: 15px; font-family: 微软雅黑">Cpp<span></span></a></li>
              
              <li><a href="/tags/catalysis/" style="font-size: 15px; font-family: 微软雅黑">catalysis<span></span></a></li>
              
              <li><a href="/tags/C/" style="font-size: 15px; font-family: 微软雅黑">C<span></span></a></li>
              
              <li><a href="/tags/chemistry/" style="font-size: 15px; font-family: 微软雅黑">chemistry<span></span></a></li>
              
              <li><a href="/tags/Parallel-Computing/" style="font-size: 15px; font-family: 微软雅黑">Parallel Computing<span></span></a></li>
              
              <li><a href="/tags/学术/" style="font-size: 15px; font-family: 微软雅黑">学术<span></span></a></li>
              
              <li class="divider"></li>
            </ul>
          </li>
          
          
		      

          <!-- Categories -->
          
          <li>
            <a href="/about" title="About me." style="font-weight: normal; font-family: Calibri,Arial; font-size: 18px">
              <i class="fa fa-user"></i>About
            </a>
          </li>
          
		      
		    </ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">		
			<h1> 实现属于自己的TensorFlow(三) - 反向传播与梯度下降实现</h1>
		</div>		
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>上一篇介绍了损失函数对计算图中节点梯度计算的方法和实现以及反向传播算法的原理，但是并没有给出如何实现损失函数从后向前计算梯度。本文主要总结在可以计算每个节点梯度的基础上如何实现反向传播计算变量的梯度以及通过反向传播算法得到的梯度实现梯度下降算法优化来优化节点中的参数。最后我们使用实现的simpleflow完成一个线性回归模型的优化。</p>
<p>附上simpleflow代码: <a target="_blank" rel="noopener" href="https://github.com/PytLab/simpleflow">https://github.com/PytLab/simpleflow</a></p>
<a id="more"></a>
<h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="反向传播计算梯度的实现"><a href="#反向传播计算梯度的实现" class="headerlink" title="反向传播计算梯度的实现"></a>反向传播计算梯度的实现</h3><p>再拿出上篇中用于解释反向传播算法的计算图:</p>
<p><img src="/assets/images/blog_img/2018-01-25-实现属于自己的TensorFlow-二-梯度计算与反向传播/bp_multiout.png" alt=""></p>
<p>而且上篇中针对图中的每个节点中输出对输入的梯度计算进行了实现，现在我们实现需要从后向前反向搜索与损失节点相联系的节点进行反向传播计算梯度。通过上面我们知道若我们需要计算其他节点关于$g$的梯度我们需要以损失节点为起点对计算图进行广度优先搜索，在搜索的过程中我们使用上篇中实现的针对每个节点的梯度计算便可以一边遍历一边计算计算节点对遍历节点的梯度了。我们可以使用一个字典将节点与梯度进行保存。</p>
<p>这里需要注意的一点是，我们在上一篇针对每个节点计算梯度是损失值$Loss$对该节点输入的梯度，用公式表示就是针对一个$f$节点的操作$y = f(x, y)$, 我们计算的梯度为$\frac{\partial Loss}{\partial x}$和$\frac{\partial Loss}{\partial y}$, 如下图:</p>
<p><img src="/assets/images/blog_img/2018-01-27-实现属于自己的TensorFlow-三-反向传播与梯度下降算法实现/node_grad.png" alt=""></p>
<p>而在广度优先搜索的时候我们访问一个节点时候计算的梯度是损失值$Loss$对<strong>此节点的输出</strong>$z$的梯度, 也就是当我们遍历访问到节点$y = f(x, y)$的时候计算的梯度为$\frac{\partial Loss}{\partial f}$或者$\frac{\partial Loss}{\partial z}$, 其中$g$为操作节点的输出节点, 如下图. 若当前访问的节点有多个输出，我们就计算损失函数$Loss$对多个输出分别的梯度再<strong>求和</strong>。</p>
<p><img src="/assets/images/blog_img/2018-01-27-实现属于自己的TensorFlow-三-反向传播与梯度下降算法实现/traversal_grad.png" alt=""></p>
<p>区别了上面的梯度，我们就可以使用Python来实现计算图的广度优先搜索了, 广度优先搜索实现这里不多做赘述了, 就是使用一个先进先出(FIFO)队列控制遍历顺序，一个集合对象存储已访问的节点防止重复访问，然后遍历的时候计算梯度并将梯度放到<code>grad_table</code>中。广度优先搜索细节可以参考<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Breadth-first_search">Breadth-first search</a> </p>
<p>梯度计算实现如下(<a target="_blank" rel="noopener" href="https://github.com/PytLab/simpleflow">https://github.com/PytLab/simpleflow</a>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_gradients</span>(<span class="params">target_op</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Backpropagation implementation computing gradient of target operation wrt</span></span><br><span class="line"><span class="string">        all the other connected nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param target_op: The target operation whose gradient wrt other nodes would</span></span><br><span class="line"><span class="string">                      be computed.</span></span><br><span class="line"><span class="string">    :type target_op: Any operation type.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return grad_table: A table containing node objects and gradients.</span></span><br><span class="line"><span class="string">    :type grad_table: dict.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># A dict containing a mapping between node and gradient value of target_op wrt the node&#x27;s output.</span></span><br><span class="line">    <span class="comment"># <span class="doctag">NOTE:</span> It is the gradient wrt the node&#x27;s OUTPUT NOT input.</span></span><br><span class="line">    grad_table = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The gradient wrt target_op itself is 1.</span></span><br><span class="line">    grad_table[target_op] = np.ones_like(target_op.output_value)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Perform a breadth-first search staring from the target_op in graph.</span></span><br><span class="line">    <span class="comment"># Queue for node traverasl.</span></span><br><span class="line">    queue = Queue()</span><br><span class="line">    queue.put(target_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set for visited nodes.</span></span><br><span class="line">    visited = set()</span><br><span class="line">    visited.add(target_op)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> queue.empty():</span><br><span class="line">        node = queue.get()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute gradient wrt the node&#x27;s output.</span></span><br><span class="line">        <span class="keyword">if</span> node != target_op:</span><br><span class="line">            grads_wrt_node_output = []</span><br><span class="line">            <span class="keyword">for</span> output_node <span class="keyword">in</span> node.output_nodes:</span><br><span class="line">                <span class="comment"># Retrieve the gradient wrt output_node&#x27;s OUTPUT.</span></span><br><span class="line">                grad_wrt_output_node_output = grad_table[output_node]</span><br><span class="line">                <span class="comment"># Compute the gradient wrt current node&#x27;s output.</span></span><br><span class="line">                grad_wrt_node_output = output_node.compute_gradient(grad_wrt_output_node_output)</span><br><span class="line">                <span class="keyword">if</span> len(output_node.input_nodes) &gt; <span class="number">1</span>:</span><br><span class="line">                    input_node_index = output_node.input_nodes.index(node)</span><br><span class="line">                    grads_wrt_node_output.append(grad_wrt_node_output[input_node_index])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    grads_wrt_node_output.append(grad_wrt_node_output)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sum all gradients wrt node&#x27;s output.</span></span><br><span class="line">            tot_grad_wrt_node_output = sum(grads_wrt_node_output)</span><br><span class="line">            grad_table[node] = tot_grad_wrt_node_output</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Put adjecent nodes to queue.</span></span><br><span class="line">        <span class="keyword">if</span> hasattr(node, <span class="string">&#x27;input_nodes&#x27;</span>):</span><br><span class="line">            <span class="keyword">for</span> input_node <span class="keyword">in</span> node.input_nodes:</span><br><span class="line">                <span class="keyword">if</span> input_node <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                    visited.add(input_node)</span><br><span class="line">                    queue.put(input_node)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grad_table</span><br></pre></td></tr></table></figure>
<h3 id="梯度下降优化"><a href="#梯度下降优化" class="headerlink" title="梯度下降优化"></a>梯度下降优化</h3><h4 id="梯度-最速下降-法"><a href="#梯度-最速下降-法" class="headerlink" title="梯度(最速下降)法"></a>梯度(最速下降)法</h4><p>上面我们实现了损失函数对其他节点梯度的计算，得到梯度的目的是为了能够优化我们的参数，本部分我们实现一个梯度下降优化器来完成参数优化的目的。关于梯度下降优化的过程其实很简单，它基于以下的观察:</p>
<blockquote>
<p>对于实值函数$F(x)$在$a$处可微且有定义，则函数$F(x)$在$a$点沿着梯度方向的反方向$-\nabla F(a)$下降最快。</p>
</blockquote>
<p>梯度下降法就是以梯度的反方向作为每轮迭代的搜索方向然后根据设定的步长对局部最优值进行搜索。 而步长就是我们的学习率。</p>
<h4 id="迭代公式"><a href="#迭代公式" class="headerlink" title="迭代公式"></a>迭代公式</h4><p>$$<br>x^{(k+1)} = x^{(k)} + \lambda_{k}d^{(k)}<br>$$</p>
<ol>
<li>搜索方向: $d^{(k)} = -\nabla f(x^{(k)})$, 也称为最速下降方向</li>
<li>搜索步长: $\lambda_{k}$, 可以固定位某值也可以变化如指数衰减或者通过一维搜索取最优步长等</li>
</ol>
<h4 id="梯度算法步骤"><a href="#梯度算法步骤" class="headerlink" title="梯度算法步骤"></a>梯度算法步骤</h4><ol>
<li>取初始点$x^{(1)} \in \mathbb{R}^n$, 允许误差$\epsilon &gt; 0$, 令$k = 1$;</li>
<li>计算搜索方向$d^{(k)} = -\nabla f(x^{(k)})$;</li>
<li>若$\lVert d^{(k)} \rVert \le \epsilon$, 则停止计算, $x^{(k)}$为所求极值点; 否则, 获取步长$\lambda_{k}$;</li>
<li>令$x^{(k+1)} = x^{(k) + \lambda_{k}d^{(k)}}$, 令$k:=k+1$转到第2步</li>
</ol>
<p>对于一个二元函数$x^{(k)}$的移动轨迹类似下图:</p>
<p><img src="/assets/images/blog_img/2018-01-27-实现属于自己的TensorFlow-三-反向传播与梯度下降算法实现/gd_2d.png" alt=""></p>
<p>对于一个一元函数的局部极小值附近$x^{(k)}$的移动轨迹类似下图:</p>
<p><img src="/assets/images/blog_img/2018-01-27-实现属于自己的TensorFlow-三-反向传播与梯度下降算法实现/gd_1d.png" alt=""></p>
<p>上面的流程中的终止条件是梯度收敛，在训练神经网络的时候我们需要根据自己的需要终止迭代，例如根据分类的准确度，均方误差或者迭代步数等.</p>
<h4 id="使用梯度下降优化计算图中的参数"><a href="#使用梯度下降优化计算图中的参数" class="headerlink" title="使用梯度下降优化计算图中的参数"></a>使用梯度下降优化计算图中的参数</h4><p>上面介绍了梯度下降算法通用的算法过程描述，这里将其应用在我们计算图中参数的优化。过程基本是一致的，只是我们现在的$x^{(k)}$变成了计算图中的参数（或者神经网络中的权重）值$W$了:</p>
<ol>
<li>随机初始化变量$W$和$b$;</li>
<li>根据$W$和$b$计算损失函数值$Loss$, 若满足终止条件，则终止迭代</li>
<li>计算损失函数$Loss$对$W$和$b$的梯度值$G_W$, $G_b$;</li>
<li>沿着梯度反方向更新$W = W - rate \times G_W$和$b = b - rate \times G_b$;</li>
<li>返回到2</li>
</ol>
<h4 id="实现梯度下降优化器"><a href="#实现梯度下降优化器" class="headerlink" title="实现梯度下降优化器"></a>实现梯度下降优化器</h4><p>好了，知道了算法的过程，我们需要给我们的simpleflow添加一个优化器，他的作用是根据当前图中节点的值计算图中所有可训练的<code>trainable</code>的变量节点中变量的梯度，并沿着梯度方向进行<strong>一步</strong>更新。</p>
<p>为了模仿tensorflow的接口，我们也定义一个<code>GradientDescentOptimizer</code>并在里面定义一个<code>minimize</code>方法来返回一个优化操作使其可以在session中执行, 代码如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GradientDescentOptimizer</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; Optimizer that implements the gradient descent algorithm.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, learning_rate</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; Construct a new gradient descent optimizer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param learning_rate: learning rate of optimizier.</span></span><br><span class="line"><span class="string">        :type learning_rate: float</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minimize</span>(<span class="params">self, loss</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27; Generate an gradient descent optimization operation for loss.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param loss: The loss operation to be optimized.</span></span><br><span class="line"><span class="string">        :type loss: Object of `Operation`</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        learning_rate = self.learning_rate</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">class</span> <span class="title">MinimizationOperation</span>(<span class="params">Operation</span>):</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">compute_output</span>(<span class="params">self</span>):</span></span><br><span class="line">                <span class="comment"># Get gradient table.</span></span><br><span class="line">                grad_table = compute_gradients(loss)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Iterate all trainable variables in graph.</span></span><br><span class="line">                <span class="keyword">for</span> var <span class="keyword">in</span> DEFAULT_GRAPH.trainable_variables:</span><br><span class="line">                    <span class="keyword">if</span> var <span class="keyword">in</span> grad_table:</span><br><span class="line">                        grad = grad_table[var]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Update its output value.</span></span><br><span class="line">                    var.output_value -= learning_rate*grad</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> MinimizationOperation()</span><br></pre></td></tr></table></figure></p>
<h3 id="使用simpleflow进行线性回归"><a href="#使用simpleflow进行线性回归" class="headerlink" title="使用simpleflow进行线性回归"></a>使用simpleflow进行线性回归</h3><p>综合前两篇以及本篇的内容，我们实现的计算图已经可以进行简单的模型训练了，本部分就以一个简单的线性模型为例子来看看成果。</p>
<h4 id="生成数据"><a href="#生成数据" class="headerlink" title="生成数据"></a>生成数据</h4><p>这里我们先基于$y=3x$生成加入一些噪声数据用于回归.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">input_x = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">input_y = input_x*<span class="number">3</span> + np.random.randn(input_x.shape[<span class="number">0</span>])*<span class="number">0.5</span></span><br></pre></td></tr></table></figure></p>
<p><img src="/assets/images/blog_img/2018-01-27-实现属于自己的TensorFlow-三-反向传播与梯度下降算法实现/data.png" alt=""></p>
<h4 id="构建计算图"><a href="#构建计算图" class="headerlink" title="构建计算图"></a>构建计算图</h4><p>下面我们来构建线性回归的计算图，线性回归的公式很简单而且只需要两个参数$w$和$b$，我们根据公式$y = wx + b$来构建计算图:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> simpleflow <span class="keyword">as</span> sf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Placeholder for training data</span></span><br><span class="line">x = sf.placeholder()</span><br><span class="line">y_ = sf.placeholder()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重参数, 因为还未实现随机初始, 目前使用1.0来初始化</span></span><br><span class="line">w = sf.Variable([[<span class="number">1.0</span>]], name=<span class="string">&#x27;weight&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 阈值</span></span><br><span class="line">b = sf.Variable(<span class="number">0.0</span>, name=<span class="string">&#x27;threshold&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测值</span></span><br><span class="line">y = x*w + b</span><br></pre></td></tr></table></figure>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>这里我们使用平方差来作为损失函数:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = sf.reduct_sum(sf.square(y - y_))</span><br></pre></td></tr></table></figure></p>
<h4 id="创建梯度下降优化器来训练模型"><a href="#创建梯度下降优化器来训练模型" class="headerlink" title="创建梯度下降优化器来训练模型"></a>创建梯度下降优化器来训练模型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">train_op = sf.GradientDescentOptimizer(learning_rate=<span class="number">0.005</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">feed_dict = &#123;x: input_x, y_: input_y&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> sf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">        loss_value = sess.run(loss, feed_dict=feed_dict)</span><br><span class="line">        mse = loss_value/len(input_x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化一步</span></span><br><span class="line">        print(<span class="string">&#x27;step: &#123;&#125;, loss: &#123;&#125;, mse: &#123;&#125;&#x27;</span>.format(step, loss_value, mse))</span><br><span class="line">        sess.run(train_op, feed_dict)</span><br><span class="line">    <span class="comment"># 训练后的参数值</span></span><br><span class="line">    w_value = sess.run(w, feed_dict=feed_dict)</span><br><span class="line">    b_value = sess.run(b, feed_dict=feed_dict)</span><br><span class="line">    print(<span class="string">&#x27;w: &#123;&#125;, b: &#123;&#125;&#x27;</span>.format(w_value, b_value))</span><br></pre></td></tr></table></figure>
<p>输出:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">step: 0, loss: 147.31229301332186, mse: 1.4731229301332187</span><br><span class="line">step: 1, loss: 75.2862864776536, mse: 0.7528628647765361</span><br><span class="line">step: 2, loss: 44.009064387253, mse: 0.44009064387253</span><br><span class="line">step: 3, loss: 30.387486500361295, mse: 0.30387486500361294</span><br><span class="line">step: 4, loss: 24.455137917985002, mse: 0.24455137917985004</span><br><span class="line">step: 5, loss: 21.871534168474543, mse: 0.21871534168474543</span><br><span class="line">step: 6, loss: 20.746346017138585, mse: 0.20746346017138584</span><br><span class="line">step: 7, loss: 20.256314070038826, mse: 0.20256314070038825</span><br><span class="line">step: 8, loss: 20.042899710055327, mse: 0.20042899710055326</span><br><span class="line">step: 9, loss: 19.949955384044085, mse: 0.19949955384044085</span><br><span class="line">step: 10, loss: 19.90947709692998, mse: 0.19909477096929978</span><br><span class="line">step: 11, loss: 19.891848352949488, mse: 0.19891848352949487</span><br><span class="line">step: 12, loss: 19.884170838991114, mse: 0.19884170838991114</span><br><span class="line">step: 13, loss: 19.880827196321707, mse: 0.19880827196321707</span><br><span class="line">step: 14, loss: 19.879371002772437, mse: 0.19879371002772436</span><br><span class="line">step: 15, loss: 19.8787368142952, mse: 0.19878736814295198</span><br><span class="line">step: 16, loss: 19.878460618163945, mse: 0.19878460618163946</span><br><span class="line">step: 17, loss: 19.878340331678682, mse: 0.19878340331678682</span><br><span class="line">step: 18, loss: 19.878287945577295, mse: 0.19878287945577294</span><br><span class="line">step: 19, loss: 19.878265130847833, mse: 0.19878265130847833</span><br><span class="line">w: [[ 2.93373825]], b: 0.04568701269996128</span><br></pre></td></tr></table></figure></p>
<h4 id="绘制回归线"><a href="#绘制回归线" class="headerlink" title="绘制回归线"></a>绘制回归线</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">w_value = float(w_value)</span><br><span class="line">max_x, min_x = np.max(input_x), np.min(input_x)</span><br><span class="line">max_y, min_y = w_value*max_x + b_value, w_value*min_x + b_value</span><br><span class="line"></span><br><span class="line">plt.plot([max_x, min_x], [max_y, min_y], color=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.scatter(input_x, input_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/assets/images/blog_img/2018-01-27-实现属于自己的TensorFlow-三-反向传播与梯度下降算法实现/line.png" alt=""></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要实现了计算图中的反向传播算法计算梯度以及梯度下降法优化参数，最后以一个线性模型为例子使用simpleflow构建的图来进行优化。后续将会在新增几个操作节点和梯度的实现来使用simpleflow构建一个多层感知机来解决简单的回归和分类问题。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a target="_blank" rel="noopener" href="http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-backpropagation/">http://www.deepideas.net/deep-learning-from-scratch-iv-gradient-descent-and-backpropagation/</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Gradient_descent">https://en.wikipedia.org/wiki/Gradient_descent</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Breadth-first_search">https://en.wikipedia.org/wiki/Breadth-first_search</a></li>
</ul>
	  
	</div>

    
	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
    	<li class="prev"><a href="/2018/03/07/遗传算法框架GAFT支持自定义个体编码方式/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/2018/01/25/实现属于自己的TensorFlow-二-梯度计算与反向传播/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>
    
	
    <!-- bdshare -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    

	<!-- comment -->
    
<section id="comment">
  <h2 class="title">Comments</h2>

  
  	 <div id="disqus_thread">
     <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  	 </div>
  
</section>

	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2018-01-27 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/代码作品/">代码作品<span class="badge">19</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/MachineLearning/">MachineLearning<span class="badge">16</span></a></li> <li><a href="/tags/TensorFlow/">TensorFlow<span class="badge">3</span></a></li> <li><a href="/tags/SimpleFlow/">SimpleFlow<span class="badge">3</span></a></li> <li><a href="/tags/NeuralNetwork/">NeuralNetwork<span class="badge">3</span></a></li> <li><a href="/tags/DeepLearning/">DeepLearning<span class="badge">3</span></a></li> <li><a href="/tags/Backpropagation/">Backpropagation<span class="badge">2</span></a></li> <li><a href="/tags/GradientDescent/">GradientDescent<span class="badge">1</span></a></li> <li><a href="/tags/ComputationalGraph/">ComputationalGraph<span class="badge">2</span></a></li>

    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-article-text">前言</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E6%AD%A3%E6%96%87"><span class="toc-article-text">正文</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-article-text">反向传播计算梯度的实现</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96"><span class="toc-article-text">梯度下降优化</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E6%A2%AF%E5%BA%A6-%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D-%E6%B3%95"><span class="toc-article-text">梯度(最速下降)法</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E8%BF%AD%E4%BB%A3%E5%85%AC%E5%BC%8F"><span class="toc-article-text">迭代公式</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="toc-article-text">梯度算法步骤</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E8%AE%A1%E7%AE%97%E5%9B%BE%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0"><span class="toc-article-text">使用梯度下降优化计算图中的参数</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-article-text">实现梯度下降优化器</span></a></li></ol></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E4%BD%BF%E7%94%A8simpleflow%E8%BF%9B%E8%A1%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-article-text">使用simpleflow进行线性回归</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE"><span class="toc-article-text">生成数据</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E6%9E%84%E5%BB%BA%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-article-text">构建计算图</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-article-text">损失函数</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E5%88%9B%E5%BB%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E5%99%A8%E6%9D%A5%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-article-text">创建梯度下降优化器来训练模型</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E7%BB%98%E5%88%B6%E5%9B%9E%E5%BD%92%E7%BA%BF"><span class="toc-article-text">绘制回归线</span></a></li></ol></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-article-text">总结</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E5%8F%82%E8%80%83"><span class="toc-article-text">参考</span></a></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->

<script type="text/javascript">
var disqus_shortname = 'pytlab';
(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  
  &copy; copyright 2020 by <a href="http://pytlab.github.io"> PytLab </a>
  
      &nbsp; <a target="_blank" rel="noopener" href="http://github.com/PytLab/hexo-theme-freemind/">Theme</a> by <a href="http://pytlab.github.io/">PytLab</a> based on <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({{ JSON.stringify(config) }});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="{{ src }}">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



</body>
   </html>
