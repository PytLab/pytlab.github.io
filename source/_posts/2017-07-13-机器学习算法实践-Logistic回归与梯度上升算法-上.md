layout: post
title: 机器学习算法实践-Logistic回归与梯度上升算法(上)
date: 2017-07-13 10:33:18
tags:
 - MachineLearning
 - LogisticRegression
 - Optimization
 - GradientAscent
 - python
categories:
 - 学习小结
feature: /assets/images/blog_img/2017-07-13-机器学习算法实践-Logistic回归与梯度上升算法-上/logistic-regression.jpg
toc: true
---

### 相关阅读
- [机器学习算法实践-决策树(Decision Tree)](http://pytlab.org/2017/07/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5-%E5%86%B3%E7%AD%96%E6%A0%91/)
- [机器学习算法实践-朴素贝叶斯(Naive Bayes)](http://pytlab.org/2017/07/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-Naive-Bayes/)

## 前言

关于Logistic回归分类器我打算用两部分总结，第一部分主要介绍Logistic回归的理论相关的部分，因为这里涉及到通过似然函数建立Logistic回归模型以及使用梯度上升算法优化参数两个主要的内容, 感觉可能比较多, 不过对于学习过最优化方法, 概率论以及线性代数的基础内容童鞋来说，这部分也是很快就掌握得啦。第二部分主要总结Logistic回归模型的代码实现和模型训练以及测试等。

## Logistic回归
Logistic回归为概率型非线性回归模型, 是研究二值型输出分类的一种多变量分析方法。通过logistic回归我们可以将二分类的观察结果$y$与一些影响因素$[x\_{1}, x\_{2}, x\_{3}, ...]$ 建立起关系从而对某些因素条件下某个结果发生的概率进行估计并分类。

<!-- more -->

## Sigmoid函数
对于二分类问题，我们想要一个函数能够接受所有输入然后预测出两种类别，可以通过输出0或者1。这个函数就是sigmoid函数，它是一种阶跃函数具体的计算公式如下:
$$
\\sigma(z) = \\frac{1}{1 + e^{-z}}
$$
Sigmoid函数的性质: 当$x$为0时，Sigmoid函数值为0.5，随着$x$的增大对应的Sigmoid值将逼近于1; 而随着$x$的减小, Sigmoid函数会趋近于0。

## Logistic回归分类器(Logistic Regression Classifier)
Logistic回归分类器是这样一种分类器:

在分类情形下，经过学习后的LR分类器是一组权值
$$
\\overline{\\omega} = [\\omega\_{0}, \\omega\_{1}, \\omega\_{2}, ... , \\omega\_{n}]^{T}
$$
，样本也可以用一组向量 $\\overline{x}$ 表示:
$$
\\overline{x} = [x\_{0}, x\_{1}, x\_{2}, x\_{3}, ... , x\_{n}]^{T}
$$
其中$x\_{0} = 1$

将 $\\overline{x}$ 根据 $\\overline{w}$ 线性叠加带入到Sigmoid函数中便可以得到范围在 $(0, 1)$, 之间的数值，大于$0.5$被分入1类，小于$0.5$的被归入0类:

$$
z = \\overline{x}^{T}\\centerdot\\overline{\\omega}
$$

$$
p(y=1 | \\overline{x}) = \\frac{1}{1 + e^{-z}}
$$

其中$p(y=1 | \\overline{x})$就是指在特征为$\\overline{x}$属于类1的条件概率, 当然也可以容易得到属于类0的概率为:

$$
p(y=0 | \\overline{x}) = 1 - p(y=1 | \\overline{x}) = \\frac{1}{1 + e^{z}}
$$

所以Logistic回归最关键的问题就是研究如何求得 $\\overline{\\omega}$ 。这个问题就需要用**似然函数**进行**极大似然**估计来处理了。

## 似然函数(Likelihood function)

> In statistics, a likelihood function (often simply the likelihood) is a function of the parameters of a statistical model given data. 

从似然函数的英文定义中可以看到，似然函数是与**统计模型中的参数**的函数。虽然似然性和概率的意思差不多，但是在统计学中却有着明确的区分:
1. 概率(Probability)使我们平时用的最多的，用于在一直某些参数的值的情况下预测某个事件被观测到的可能性。
2. 似然性(Likelihood)则是在一直观测到的结果时，对有关参数进行估计。

可见这两个是个概念是个可逆的过程，即似然函数是条件概率的逆反.

对于某个已发生的事件 $x$, 某个参数或者某个参数向量 $\\theta$ 的似然函数的值与已知参数 $\\theta$ 前提下相同事件 $x$ 放生的条件概率(概率密度)的值相等, 即:

$$
\\mathcal{L}(\\theta | x) = P(x | \\theta)
$$

似然函数对于离散和连续随机分布的表示形式是不同的:

### 离散型
对于具有与参数 $\\theta$ 相关离散概率分布 $p$ 的变量 $X$, 对于某个变量 $X = x$ $\\theta$ 的似然函数表示成:
$$
\\mathcal{L}(\\theta | x) = p\_{\\theta}(x) = P\_{\\theta}(X=x)
$$

### 连续型
连续性的分布我们则用概率密度 $f$ 来表示:
$$
\\mathcal{L}(\\theta | x) = f\_{\\theta}(x)
$$
注意似然函数并不是一个条件概率，虽然表达式与条件概率的形式相同。因为 $\\theta$ 并不是一个随机变量而是一个参数。

{% alert info %}
关于对似然性的理解，个人认为，似然性并不是一个概率，而是表示在一些列事件发生时，关于事件相关的参数的<b>可能性信息</b>，一个参数就对应一个似然函数的值，当参数发生变化的时候，似然函数也会随之变化，<b>似然函数的重要性并不在于他的具体值是多少，而在于他随参数变化的变化趋势</b>，是变大还是变小。当我们在取得某个参数的时候，似然函数的值到达了极大值，则说明这个参数具有**最合理性**。
{% endalert %}

### 极大似然估计
极大似然估计是似然函数最初也是最然的应用，我们优化Logistic模型就行极大似然估计的过程(求似然函数的极大值)，通过极大似然估计，我们可以得到最合理的参数。

## Logistic回归中的极大似然估计
上一部分总结了什么似然函数和极大似然估计，这里就总结下Logistic模型的极大似然估计。

在LR分类器部分我们推导了Sigmoid函数计算两类问题的概率表达式，由于是二分类，分类结果是0和1，我们可以将两种类别的概率用一个式子表达, 对于一个样本 $\\overline{x\_{i}}$ 得到一个观测值为 $y\_{i}$ 的概率为:
$$
P(y = y\_{i} | \\overline{x\_{i}}) = p(y = 1 | \\overline{x\_{i}})^{y\_{i}}(1 - p(y = 0 | \\overline{x\_{i}})^{1 - y\_{i}} = (\\frac{1}{1 + e^{-\\overline{x\_{i}}\\centerdot\\overline{\\omega}^{T}}})^{y\_{i}}(\\frac{1}{1 + e^{\\overline{x\_{i}}\\centerdot\\overline{\\omega}^{T}}})^{1 - y\_{i}}
$$

若各个样本之间是相互独立的，则联合概率为各个样本概率的乘积。于是根据这系列的样本，我们就能得到关于参数向量 $\\overline{\\omega}$ 的似然函数 $\\mathcal{L}(\\overline{\\omega})$ :

$$
\\mathcal{L}(\\overline{\\omega}) = \\prod\_{i=1}^{n}(\\frac{1}{1 + e^{-\\overline{x\_{i}}\\centerdot\\overline{\\omega}^{T}}})^{y\_{i}}(\\frac{1}{1 + e^{\\overline{x\_{i}}\\centerdot\\overline{\\omega}^{T}}})^{1 - y\_{i}}
$$

我们的目的就是要对这个似然函数的极大值进行参数估计，这便是我们训练Logistic回归模型的过程。通过极大似然估计我们便可以通过所有样本得到满足训练数据集的最合理的参数 $\\overline{\\omega}$

## 通过梯度上升算法进行极大似然估计

有了似然函数，我们便可以通过优化算法来进行优化了。使用梯度上升需要计算目标函数的梯度，下面我简单对梯度的计算进行一下推导:

为了方便，我们将似然函数取自然对数先,

$$
ln\\mathcal{L}(\\overline{\\omega}) = \\sum\_{i=1}^{n}[y\_{i}\\centerdot ln\\frac{exp(x\_{i})}{exp(x\_{i} + 1)} + (1 - y\_{i}\\centerdot ln\\frac{1}{exp(x\_{i} + 1)})]
$$
$$
ln\\mathcal{L}(\\overline{\\omega}) = \\sum\_{i=1}^{m}[y\_{i} \\centerdot ln(exp(x\_{i})) - y\_{i} \\centerdot ln(1 + exp(x\_{i})) - (1 - y\_{i})ln(1 + exp(x\_{i}))]
$$
$$
ln\\mathcal{L}(\\overline{\\omega}) = \\sum\_{i=1}^{n}(x\_{i}y\_{i} - ln(1 + exp(x\_{i})))
$$

然后我们对去过对数的函数的梯度进行计算:

$$
\\frac{\\partial ln \\mathcal{L}(\\overline{\\omega})}{\\partial \\omega\_{k}} = \\sum\_{i=1}^{m}x\_{ik}(y\_{i} - \\frac{1}{1 + exp(\\overline{\\omega} \\centerdot \\overline{x\_{i}})})
$$

通过矩阵乘法直接表示成梯度:

$$
\\nabla ln\\mathcal{L}(\\overline{\\omega}) = \\overline{x} \\centerdot (\\overline{y} - \\overline{\\pi(\\overline{x})}) = \\overline{x} \\centerdot \\overline{error}
$$

设步长为 \\alpha, 则迭代得到的新的权重参数为:

$$
\\overline{w} := \\overline{w} + \\alpha \\nabla ln\\mathcal{L}(\\overline{\\omega})
$$

这样我们通过梯度上升法做极大似然估计来做Logistic回归的过程就很清楚了，剩下的我们就需要通过代码来实现Logistic回归吧.

## 总结
本文主要总结了Logistic回归相关的原理以及使用梯度上升法做回归优化的过程推导，为我们自己实现Logistic回归分类器做好了充足的准备。

## 参考
- https://en.wikipedia.org/wiki/Likelihood_function
- 《Machine Learning in Action》
